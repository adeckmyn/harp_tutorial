[
["index.html", "harp Tutorial Chapter 1 Getting Started 1.1 System Requirements 1.2 Installation 1.3 Setting up a project 1.4 Learning R 1.5 Isolating harp", " harp Tutorial Andrew Singleton 2020-08-06 Chapter 1 Getting Started 1.1 System Requirements 1.1.1 R harp has been tested on Linux platforms with R versions going back to R 3.3.1. At the time of writing, the current version of R is R 4.0.1. See https://cran.r-project.org/ for details on how to download and install R. 1.1.2 System libraries harp depends on some system libraries that may not be installed by default. We are working on identifying all of these, but the most common missing library is PROJ. On Ubuntu this can be installed with sudo apt-get update sudo apt-get install libproj-dev If you do not have sudo rights, speak to your system administrator, or install proj in a location that is writable to you following the instructions at https://proj.org/install.html. Note that for guaranteed performance you should use version 4.9.3. If you wish to use NetCDF files with harp you will also need to install libnetcdf-dev using apt-get install or the equivalent for your system. If you wish to use grib files you will need to install eccodes - for Ubuntu 18 and newer, this is available via apt-get install libeccodes-dev, otherwise see https://confluence.ecmwf.int/display/ECC. 1.1.3 RStudio RStudio is an IDE for R that makes working in R much easier. It is recommended that you work with RStudio for harp, though it is possible to work in a terminal and use any editor for writing scripts. Go to https://rstudio.com/products/rstudio/ to download RStudio. 1.2 Installation harp is stored on Github, which makes installation within R straightforward. There are a number of dependencies that need to be downloaded and compiled, so if you have a new R installation the process can take some time. First you need to start R, or RStudio and install the \"remotes\" package using install.packages. If you are concerned about updating R packages that you use for other projects then skip the rest of this section and go to the section on Isolating harp. install.packages(&quot;remotes&quot;) Then we can use the remotes package to install harp from Github: remotes::install_github(&quot;harphub/harp&quot;) If you encounter problems with system libraries, the missing libraries will make themselves known at this stage. If you have system libraries installed in non-standard locations, you can specify those locations using the configure.args argument in combination with the R package that needs the system library. For example, if you have PROJ, which is required by the meteogrid package, installed in a non-standard location, you would do the following: remotes::install_github( &quot;harphub/harp&quot;, configure.args = c( meteogrid = &quot;--with-proj=/path/to/proj&quot; ) ) where /path/to/proj is the full path to your PROJ installation. 1.3 Setting up a project To follow the tutorial, you will need to download the data. These data are found in the harpData package, which can also be installed from Github: remotes::install_github(&quot;harphub/harpData&quot;) For the tutorial you should set up a project in clean directory. There are slightly different approaches depending on whether you’re using RStudio or R from the terminal 1.3.1 RStudio Click on: | File &gt; New Project Select: | - New Directory | - New Project Choose a directory name (e.g. harp_tutorial) Browse to a directory under which your project should reside Click on: | Create Project 1.3.2 From the terminal Open R and do the following: project_dir &lt;- &quot;/path/to/harp_tutorial&quot; dir.create(project_dir, recursive = TRUE) setwd(project_dir) Alternatively, you can create the directory outside of R and start R from that directory. 1.3.3 Linking to the data We will use the here package to set the base directory of our project. This means that whatever directory we are currently in within the project, we can set relative paths to any files and directories in the project. We also make use of the tidyverse for some data manipulation functions. install.packages(&quot;here&quot;) install.packages(&quot;tidyverse&quot;) library(here) Now will create a data directory and link the contents of the harpData package to the data directory. We will do this by making a vector of the directories we want and loop over the vector and create the directories. Don’t worry too much about the syntax at this stage. dir.create(here(&quot;data&quot;)) for (data_dir in c(&quot;grib&quot;, &quot;netcdf&quot;, &quot;vfld&quot;, &quot;vobs&quot;)) { file.symlink(from = system.file(data_dir, package = &quot;harpData&quot;), to = here(&quot;data&quot;)) } You are now ready to start the first tutorial! 1.4 Learning R It is not the purpose of this tutorial to teach R, but rather to teach the usage of harp in R. While it is not necessary to know R to learn how to use harp, a basic understanding of the language would be useful. One of the best resources for getting started with R is the book “R for Data Science” by Hadley Wickham, which is available for free online at https://r4ds.had.co.nz/. This book makes use of what is known as the “tidyverse” in R, and harp follows many of the same principles so the book is an extremely useful learning resource for harp. 1.5 Isolating harp If you use R for other projects that are sensitive to package versions, and thus don’t want to risk updating those packages when you install harp, you can install harp as a project with its own package repository. This is done using the renv package. In RStudio this is straightforward - when you create the project, tick the box labeled “Use renv with this project” (for older versions of RStudio this will be “Use Packrat with this project”) before clicking on the final “Create Project” button. If you are using R from the terminal, before installing harp create the project directory as described in R from the terminal and navigate to it using setwd. Then use renv to create the isolated environment: install.packages(&quot;renv&quot;) renv::init() renv::install(&quot;harphub/harp&quot;) renv::install(&quot;tidyverse&quot;) renv::install(&quot;here&quot;) renv::snapshot() You will need to do this for every harp based project, but it will ensure that you do not overwrite any version sensitive packages. "],
["interpolating-model-outputs.html", "Chapter 2 Interpolating model outputs 2.1 Introduction 2.2 File templates 2.3 Deterministic forecast data 2.4 Ensemble forecast data", " Chapter 2 Interpolating model outputs 2.1 Introduction The “internal” file format for harp is SQLite. SQLite allows fast access to the data and the ability to filter exactly what you want to read. Once our data are in SQLite format, we don’t have to convert them again. In most cases you also need to interpolate forecast data to station locations, and harp enables you to read the data, interpolate it, and write the interpolated data in one function. This can be done for grib files, FA files, netcdf files (from MET Norway) and vfld files (which are already interpolated). All of the file IO in harp is handled by the package harpIO. 2.2 File templates NWP files typically have information from any of the date, lead time, parameter, member number (for ensemble forecasts) and potentially more in the file name and / or directory structure. This means that the exact file names are never the same though they will always have the same structure. harp provides functionality for using templates for file names, and also has a number of built in templates for known formats. You can list these using show_file_templates() and see the way that the templates are built. library(harp) library(here) show_file_templates() ## # A tibble: 31 x 2 ## template_name template ## &lt;chr&gt; &lt;chr&gt; ## 1 arome_arctic_extrac… /lustre/storeB/immutable/archive/projects/metproduction… ## 2 arome_arctic_full /lustre/storeB/immutable/archive/projects/metproduction… ## 3 arome_arctic_sfx /lustre/storeB/immutable/archive/projects/metproduction… ## 4 fctable {file_path}/{fcst_model}/{YYYY}/{MM}/FCTABLE_{parameter… ## 5 fctable_det {file_path}/{det_model}/{YYYY}/{MM}/FCTABLE_{parameter}… ## 6 fctable_eps {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{parameter}… ## 7 fctable_eps_all_cyc… {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{parameter}… ## 8 fctable_eps_all_lea… {file_path}/{eps_model}/{YYYY}/{MM}/FCTABLE_{parameter}… ## 9 glameps_grib {file_path}/{eps_model}/{sub_model}/{YYYY}/{MM}/{DD}/{H… ## 10 harmoneps_grib {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{MM… ## 11 harmoneps_grib_fp {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{MM… ## 12 harmoneps_grib_sfx {file_path}/{YYYY}/{MM}/{DD}/{HH}/mbr{MBR3}/fc{YYYY}{MM… ## 13 harmonie_grib {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH}+… ## 14 harmonie_grib_fp {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH}+… ## 15 harmonie_grib_sfx {file_path}/{YYYY}/{MM}/{DD}/{HH}/fc{YYYY}{MM}{DD}{HH}+… ## 16 meps_cntrl_extracted /lustre/storeB/immutable/archive/projects/metproduction… ## 17 meps_cntrl_sfx /lustre/storeB/immutable/archive/projects/metproduction… ## 18 meps_extracted /lustre/storeB/immutable/archive/projects/metproduction… ## 19 meps_full /lustre/storeB/immutable/archive/projects/metproduction… ## 20 meps_sfx /lustre/storeB/immutable/archive/projects/metproduction… ## 21 meps_subset /lustre/storeB/immutable/archive/projects/metproduction… ## 22 obstable {file_path}/OBSTABLE_{YYYY}.sqlite ## 23 vfld {file_path}/{fcst_model}/vfld{fcst_model}{YYYY}{MM}{DD}… ## 24 vfld_det {file_path}/{det_model}/vfld{det_model}{YYYY}{MM}{DD}{H… ## 25 vfld_det_noexp {file_path}/{det_model}/vfld{YYYY}{MM}{DD}{HH}{LDT2} ## 26 vfld_eps {file_path}/{sub_model}/vfld{sub_model}mbr{MBR3}{YYYY}{… ## 27 vfld_eps_noexp {file_path}/{sub_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{HH}… ## 28 vfld_multimodel {file_path}/{sub_model}/vfld{sub_model}mbr{MBR3}{YYYY}{… ## 29 vfld_multimodel_noe… {file_path}/{sub_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{HH}… ## 30 vfld_noexp {file_path}/{fcst_model}/vfldmbr{MBR3}{YYYY}{MM}{DD}{HH… ## 31 vobs {file_path}/vobs{YYYY}{MM}{DD}{HH} Since the output is typically truncated you can see one of the templates in full by supplying the row number in the original output. For example, to see the template for vfld files show_file_templates(23) ## ## template_name: ## vfld ## ## template: ## {file_path}/{fcst_model}/vfld{fcst_model}{YYYY}{MM}{DD}{HH}{LDT2} 2.3 Deterministic forecast data Let’s go ahead and read some data. In the data directory you will find a vfld directory and there you will find a directory for AROME_Arctic_prod. Under that directory you will find vfld files for one day of forecasts. To read the date we will use the function read_forecast. Each of the arguments are annotated so that you understand what they are for. read_forecast( start_date = 2019021700, # the first forecast for which we have data end_date = 2019021718, # the last forecast for which we have data fcst_model = &quot;AROME_Arctic_prod&quot;, # the name of the deterministic model as in the file name parameter = &quot;T2m&quot;, # We are going to read 2m temperature lead_time = seq(0, 48, 3), # We have data for lead times 0 - 48 at 3 hour intervals by = &quot;6h&quot;, # We have forecasts every 6 hours file_path = here(&quot;data/vfld&quot;), # We don&#39;t include AROME_Arctic_prod in the path... file_template = &quot;vfld&quot;, # ...because it&#39;s in the template return_data = TRUE # We want to get some data back - by default nothing is returned ) ## ● AROME_Arctic_prod ## # A tibble: 16,796 x 10 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1002 80.1 16.2 K ## 3 2019-02-17 00:00:00 0 T2m 1003 77 15.5 K ## 4 2019-02-17 00:00:00 0 T2m 1004 78.9 11.9 K ## 5 2019-02-17 00:00:00 0 T2m 1006 78.3 22.8 K ## 6 2019-02-17 00:00:00 0 T2m 1007 78.9 11.9 K ## 7 2019-02-17 00:00:00 0 T2m 1008 78.2 15.5 K ## 8 2019-02-17 00:00:00 0 T2m 1009 80.7 25.0 K ## 9 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 10 2019-02-17 00:00:00 0 T2m 1011 80.1 31.5 K ## # … with 16,786 more rows, and 3 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, AROME_Arctic_prod_det &lt;dbl&gt; We don’t have to only handle one model at a time, as long as they have the same file format and fit the same template. We also have data for MEPS, which is an ensemble, but we can just read member 0 by setting the correct template. When there are different options for each fcst_model they can be passed as a named list as below: read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), parameter = &quot;T2m&quot;, lead_time = seq(0, 48, 3), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( AROME_Arctic_prod = &quot;vfld&quot;, MEPS_prod = &quot;{fcst_model}/vfld{fcst_model}mbr000{YYYY}{MM}{DD}{HH}{LDT2}&quot; ), return_data = TRUE ) ## ● AROME_Arctic_prod ## # A tibble: 16,796 x 10 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1002 80.1 16.2 K ## 3 2019-02-17 00:00:00 0 T2m 1003 77 15.5 K ## 4 2019-02-17 00:00:00 0 T2m 1004 78.9 11.9 K ## 5 2019-02-17 00:00:00 0 T2m 1006 78.3 22.8 K ## 6 2019-02-17 00:00:00 0 T2m 1007 78.9 11.9 K ## 7 2019-02-17 00:00:00 0 T2m 1008 78.2 15.5 K ## 8 2019-02-17 00:00:00 0 T2m 1009 80.7 25.0 K ## 9 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 10 2019-02-17 00:00:00 0 T2m 1011 80.1 31.5 K ## # … with 16,786 more rows, and 3 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, AROME_Arctic_prod_det &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 78,132 x 10 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 3 2019-02-17 00:00:00 0 T2m 1014 69.2 17.9 K ## 4 2019-02-17 00:00:00 0 T2m 1015 69.6 17.8 K ## 5 2019-02-17 00:00:00 0 T2m 1018 69.2 16.0 K ## 6 2019-02-17 00:00:00 0 T2m 1023 69.1 18.5 K ## 7 2019-02-17 00:00:00 0 T2m 1025 69.7 18.9 K ## 8 2019-02-17 00:00:00 0 T2m 1026 69.7 18.9 K ## 9 2019-02-17 00:00:00 0 T2m 1027 69.7 18.9 K ## 10 2019-02-17 00:00:00 0 T2m 1033 70.2 19.5 K ## # … with 78,122 more rows, and 3 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, MEPS_prod_det &lt;dbl&gt; If we want to write the data to sqlite files we need to tell it a path to write the data to. If the directories do not exist, they will be created. Let’s write the data to “data/FCTABLE”. We use FCTABLE as the directory name since in harp we refer to the SQLite files created by this process as FCTABLE files. We tell read_forecast that we want to output the data to files by setting output_file_opts to something. For SQLite files we can use the sqlite_opts function to set the options we need - usually you only need to set the path to the data using the path argument. Let’s get the vertical temperature profiles as well as the 2m temperature. You can see the parameter names that harp understands by running the function show_harp_parameters show_harp_parameters() ## # A tibble: 21 x 2 ## harp_parameter_name description ## &lt;chr&gt; &lt;chr&gt; ## 1 AccPcp12h Accumulated precipitation over e.g. 12 hours ## 2 Cbase Height of cloud base ## 3 CChigh High level cloud cover ## 4 CClow Low level cloud cover ## 5 CCmed Medium level cloud cover ## 6 CCtot Total cloud cover ## 7 D10m 10m wind direction ## 8 G10m 10m wind gust - period depends on input data ## 9 Gmax 10m maximum wind gust - period depends on input data ## 10 Pcp Precipitation direct from model - usually accumulated fr… ## 11 Pmsl Pressure at mean sea level ## 12 Ps Pressure at surface ## 13 Q2m 2m specific humidity ## 14 RH2m 2m relative humidity ## 15 S10m 10m wind speed ## 16 Smax Maximum 10m wind speed - period depends on input data ## 17 T2m 2m temperature ## 18 Td2m 2m dewpoint temperature ## 19 Tmax Maximum 2m temperature ## 20 Tmin Minimum 2m temperature ## 21 vis Horizontal visibility ## ## For upper air parameters, Z, T, RH, D, S, Q, and Td are available. Follow the ## letter with a number to denote pressure level, e.g. T850, S925, Z300 etc. So the upper air parameter for temperture is T. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), parameter = c(&quot;T2m&quot;, &quot;T&quot;), lead_time = seq(0, 48, 3), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( AROME_Arctic_prod = &quot;vfld&quot;, MEPS_prod = &quot;{fcst_model}/vfld{fcst_model}mbr000{YYYY}{MM}{DD}{HH}{LDT2}&quot; ), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/deterministic&quot;)) ) The function tells you where the data were written to, but you can also look under the ./data/FCTABLE/deterministic directory and see for yourself. By default read_forecast uses the “fctable” template, but this can be changed using the template argument in sqlite_opts. However, you should always keep the parameter name in the template. There are a number of options in read_forecast that can be explored by looking at the help page for the function. Your turn: 2m temperature is, by default, corrected for the difference in elevation between the model and observation site. Create sqlite files for uncorrected 2m temperature for AROME_Arctic_prod and MEPS_prod (member 0) Create sqlite files for 10m wind speed and dew point temperature for the upper air for AROME_Arctic_prod and MEPS_prod (member 0) Solutions Create sqlite files for uncorrected 2m temperature for AROME_Arctic_prod and MEPS_prod (member 0) read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), parameter = &quot;T2m&quot;, lead_time = seq(0, 48, 3), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( AROME_Arctic_prod = &quot;vfld&quot;, MEPS_prod = &quot;{fcst_model}/vfld{fcst_model}mbr000{YYYY}{MM}{DD}{HH}{LDT2}&quot; ), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/deterministic&quot;)), transformation_opts = interpolate_opts(keep_model_t2m = TRUE) ) Create sqlite files for 10m wind speed and dew point temperature for the upper air for AROME_Arctic_prod and MEPS_prod (member 0) read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), parameter = c(&quot;S10m&quot;, &quot;Td&quot;), lead_time = seq(0, 48, 3), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( AROME_Arctic_prod = &quot;vfld&quot;, MEPS_prod = &quot;{fcst_model}/vfld{fcst_model}mbr000{YYYY}{MM}{DD}{HH}{LDT2}&quot; ), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/deterministic&quot;)) ) 2.4 Ensemble forecast data The interpolation of EPS model data works in much the same way as for deterministic model data, using the read_forecast function. By adding the members argument, we tell read_forecast that we are reading ensemble data. 2.4.1 Read and interpolate 2m temperature As mentioned in the deterministic section, there are vfld files for MEPS in the data directory. There are 10 members numbered from 0 to 9, but we only have up to a lead time of 12 hours. As for the deterministic model, let’s first get the 2m temperature library(tidyverse) library(here) library(harpIO) read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;MEPS_prod&quot;), parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), members = seq(0, 9), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = &quot;vfld_eps&quot;, return_data = TRUE ) ## ● MEPS_prod ## # A tibble: 22,980 x 19 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 3 2019-02-17 00:00:00 0 T2m 1014 69.2 17.9 K ## 4 2019-02-17 00:00:00 0 T2m 1015 69.6 17.8 K ## 5 2019-02-17 00:00:00 0 T2m 1018 69.2 16.0 K ## 6 2019-02-17 00:00:00 0 T2m 1023 69.1 18.5 K ## 7 2019-02-17 00:00:00 0 T2m 1025 69.7 18.9 K ## 8 2019-02-17 00:00:00 0 T2m 1026 69.7 18.9 K ## 9 2019-02-17 00:00:00 0 T2m 1027 69.7 18.9 K ## 10 2019-02-17 00:00:00 0 T2m 1033 70.2 19.5 K ## # … with 22,970 more rows, and 12 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, ## # MEPS_prod_mbr002 &lt;dbl&gt;, MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, ## # MEPS_prod_mbr005 &lt;dbl&gt;, MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, ## # MEPS_prod_mbr008 &lt;dbl&gt;, MEPS_prod_mbr009 &lt;dbl&gt; You will see that there are a few extra columns in the data. There is both fcst_model and sub_model. This is in case we are dealing with a multi model ensemble. There is also a column for member, and members_out. This is if you want to renumber the members for the sqlite files. Finally there is a lag column. This has information for lagged ensembles. Let’s now try reading two models - the AROME_Arctic_prod deterministic model and the MEPS_prod ensemble. But how do we specify the members for more than one model? In this case we use a named list, with one element of the list for each model - and we can do the same for the file template. By setting the members to NA for AROME_Arctic_prod we tell read_forecast that it is deterministic - you could also set it to a numeric value and it would treat the AROME_Arctic_prod as a single member ensemble. We’ll just take the first 3 members from MEPS_prod for speed. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;MEPS_prod&quot;, &quot;AROME_Arctic_prod&quot;), parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), members = list(MEPS_prod = seq(0, 2), AROME_Arctic_prod = NA), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list(MEPS_prod = &quot;vfld_eps&quot;, AROME_Arctic_prod = &quot;vfld&quot;), return_data = TRUE ) ## ● AROME_Arctic_prod ## # A tibble: 4,940 x 10 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1002 80.1 16.2 K ## 3 2019-02-17 00:00:00 0 T2m 1003 77 15.5 K ## 4 2019-02-17 00:00:00 0 T2m 1004 78.9 11.9 K ## 5 2019-02-17 00:00:00 0 T2m 1006 78.3 22.8 K ## 6 2019-02-17 00:00:00 0 T2m 1007 78.9 11.9 K ## 7 2019-02-17 00:00:00 0 T2m 1008 78.2 15.5 K ## 8 2019-02-17 00:00:00 0 T2m 1009 80.7 25.0 K ## 9 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 10 2019-02-17 00:00:00 0 T2m 1011 80.1 31.5 K ## # … with 4,930 more rows, and 3 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, AROME_Arctic_prod_det &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 22,980 x 12 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 3 2019-02-17 00:00:00 0 T2m 1014 69.2 17.9 K ## 4 2019-02-17 00:00:00 0 T2m 1015 69.6 17.8 K ## 5 2019-02-17 00:00:00 0 T2m 1018 69.2 16.0 K ## 6 2019-02-17 00:00:00 0 T2m 1023 69.1 18.5 K ## 7 2019-02-17 00:00:00 0 T2m 1025 69.7 18.9 K ## 8 2019-02-17 00:00:00 0 T2m 1026 69.7 18.9 K ## 9 2019-02-17 00:00:00 0 T2m 1027 69.7 18.9 K ## 10 2019-02-17 00:00:00 0 T2m 1033 70.2 19.5 K ## # … with 22,970 more rows, and 5 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, ## # MEPS_prod_mbr002 &lt;dbl&gt; We could also generate a multimodel ensemble. Here we use a named nested list for members and a named list for fcst_model. Note also that we have to set custom template for AROME_Arctic_prod since for multimodel ensembles to work, {sub_model} needs to be a part of the file template. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = list(awesome_multimodel_eps = c(&quot;MEPS_prod&quot;, &quot;AROME_Arctic_prod&quot;)), parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), members = list(awesome_multimodel_eps = list(MEPS_prod = seq(0, 2), AROME_Arctic_prod = 0)), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( awesome_mutlimodel_eps = list( MEPS_prod = &quot;vfld_multimodel&quot;, AROME_Arctic_prod = &quot;{sub_model}/vfld{sub_model}{YYYY}{MM}{DD}{HH}{LDT2}&quot; ) ), return_data = TRUE, ) ## ● awesome_multimodel_eps ## # A tibble: 23,340 x 13 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 3 2019-02-17 00:00:00 0 T2m 1014 69.2 17.9 K ## 4 2019-02-17 00:00:00 0 T2m 1015 69.6 17.8 K ## 5 2019-02-17 00:00:00 0 T2m 1018 69.2 16.0 K ## 6 2019-02-17 00:00:00 0 T2m 1023 69.1 18.5 K ## 7 2019-02-17 00:00:00 0 T2m 1025 69.7 18.9 K ## 8 2019-02-17 00:00:00 0 T2m 1026 69.7 18.9 K ## 9 2019-02-17 00:00:00 0 T2m 1027 69.7 18.9 K ## 10 2019-02-17 00:00:00 0 T2m 1033 70.2 19.5 K ## # … with 23,330 more rows, and 6 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, AROME_Arctic_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr000 &lt;dbl&gt;, ## # MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt; You will see that the members are named differently for each of the models in the ensemble. You can also use the split_multimodel function to separate out each of the sub models into their own data frames in the harp_fcst list. Your turn: Create sqlite files for all members of MEPS_prod for lead times 0, 3, 6, 9 and 12 for 2m temperature (both corrected and uncorrected), 10m wind speed and temperature and dew point temperature for the upper air. Make a multimodel ensemble with members 0-5 of MEPS_prod and AROME_Arctic_prod as a single member ensemble and write out the output to sqlite files for the same lead times and parameters as above. Note that to ensure unique rows in the sqlite files we cannot write out the model elevation. See ?sqlite_opts for how to control this behaviour. Solutions Create sqlite files for all members of MEPS_prod for lead times 0, 3, 6, 9 and 12 for 2m temperature (both corrected and uncorrected), 10m wind speed and temperature and dew point temperature for the upper air. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;MEPS_prod&quot;, parameter = c(&quot;T2m&quot;, &quot;S10m&quot;, &quot;T&quot;, &quot;Td&quot;), lead_time = seq(0, 12, 3), members = seq(0, 9), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = &quot;vfld_eps&quot;, transformation_opts = interpolate_opts(keep_model_t2m = TRUE), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/ensemble&quot;)) ) Make a multimodel ensemble with members 0-5 of MEPS_prod and AROME_Arctic_prod and write out the output to sqlite files for the same lead times and parameters as above. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = list(awesome_multimodel_eps = c(&quot;MEPS_prod&quot;, &quot;AROME_Arctic_prod&quot;)), parameter = c(&quot;T2m&quot;, &quot;S10m&quot;, &quot;T&quot;, &quot;Td&quot;), lead_time = seq(0, 12, 3), members = list(awesome_multimodel_eps = list(MEPS_prod = seq(0, 2), AROME_Arctic_prod = 0)), by = &quot;6h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = list( awesome_multimodel_eps = list( MEPS_prod = &quot;vfld_multimodel&quot;, AROME_Arctic_prod = &quot;{sub_model}/vfld{sub_model}{YYYY}{MM}{DD}{HH}{LDT2}&quot; ) ), transformation_opts = interpolate_opts(keep_model_t2m = TRUE), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/ensemble&quot;), remove_model_elev = TRUE) ) 2.4.2 Lagged ensembles In a number of Hirlam institutes (MetCoOp and DMI) “continuous” ensembles are being run. With these a small number of members are run each hour and the full ensemble is constructed by lagging these hourly members. In MetCoOp this ensemble is known as CMEPS and is currently running in test mode. We have some vfld files for CMEPS_prod in the data directory for the same dates as the other modelling systems. Each member runs every three hours, with members 1 and 2 at 00, 03…, 21; members 5 and 6 at 01, 04…, 22; and members 3 and 4 at 02, 05…, 23. To construct the ensemble for 03, we need members from 03, 02 and 01. We specifiy what lags we need as a named list that is the same length as the members argument. read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;CMEPS_prod&quot;, parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), members = c(0, 1, 3, 4, 5, 6), lags = c(0, 0, 2, 2, 1, 1), by = &quot;3h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = &quot;vfld_eps&quot;, return_data = TRUE ) ## ● CMEPS_prod ## # A tibble: 40,215 x 15 ## fcdate lead_time parameter SID lat lon units ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-02-17 00:00:00 0 T2m 1001 70.9 -8.67 K ## 2 2019-02-17 00:00:00 0 T2m 1010 69.3 16.1 K ## 3 2019-02-17 00:00:00 0 T2m 1014 69.2 17.9 K ## 4 2019-02-17 00:00:00 0 T2m 1015 69.6 17.8 K ## 5 2019-02-17 00:00:00 0 T2m 1018 69.2 16.0 K ## 6 2019-02-17 00:00:00 0 T2m 1023 69.1 18.5 K ## 7 2019-02-17 00:00:00 0 T2m 1025 69.7 18.9 K ## 8 2019-02-17 00:00:00 0 T2m 1026 69.7 18.9 K ## 9 2019-02-17 00:00:00 0 T2m 1027 69.7 18.9 K ## 10 2019-02-17 00:00:00 0 T2m 1033 70.2 19.5 K ## # … with 40,205 more rows, and 8 more variables: validdate &lt;dttm&gt;, ## # fcst_cycle &lt;chr&gt;, CMEPS_prod_mbr000 &lt;dbl&gt;, CMEPS_prod_mbr001 &lt;dbl&gt;, ## # CMEPS_prod_mbr003_lag2h &lt;dbl&gt;, CMEPS_prod_mbr004_lag2h &lt;dbl&gt;, ## # CMEPS_prod_mbr005_lag1h &lt;dbl&gt;, CMEPS_prod_mbr006_lag1h &lt;dbl&gt; Your turn Get CMEPS_prod data for all of the same parameters as before and write to SQLite files Do the same for precipitation (“Pcp”) for both CMEPS_prod and MEPS_prod Solutions Get CMEPS_prod data for all of the same parameters as before and write to SQLite files read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;CMEPS_prod&quot;, parameter = c(&quot;T2m&quot;, &quot;S10m&quot;, &quot;T&quot;, &quot;Td&quot;), lead_time = seq(0, 12, 3), members = c(0, 1, 3, 4, 5, 6), lags = c(0, 0, 2, 2, 1, 1), by = &quot;3h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = &quot;vfld_eps&quot;, transformation_opts = interpolate_opts(keep_model_t2m = TRUE), output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/ensemble&quot;)) ) Do the same for precipitation (“Pcp”) for both CMEPS_prod and MEPS_prod read_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;MEPS_prod&quot;, &quot;CMEPS_prod&quot;), parameter = &quot;Pcp&quot;, lead_time = seq(0, 12, 3), members = list(MEPS_prod = seq(0, 9), CMEPS_prod = c(0, 1, 3, 4, 5, 6)), lags = list(CMEPS_prod = c(0, 0, 2, 2, 1, 1)), by = &quot;3h&quot;, file_path = here(&quot;data/vfld&quot;), file_template = &quot;vfld_eps&quot;, output_file_opts = sqlite_opts(path = here(&quot;data/FCTABLE/ensemble&quot;)), ) "],
["convert-observations-to-sqlite.html", "Chapter 3 Convert observations to SQLite 3.1 Introduction 3.2 Converting observations to SQLite", " Chapter 3 Convert observations to SQLite 3.1 Introduction Point observations also come in many formats. For Hirlam, that format is vobs. Currently this is the only point observations format that harp can deal with. vobs are pretty much the same format as vfld, with files for every observation time. This is considerable file IO if you want a season’s worth of data, for example. Again in harp we use sqlite to store point observations, making it quick and easy to access exactly what you want. 3.2 Converting observations to SQLite The harp function for converting observations is read_obs_convert. It works similarly to the read_forecast function, but has fewer arguments. In the data directory is a vobs directory containing the vobs files. We have hourly data here from 00 UTC 17 Feb 2019 to 23 UTC 20 Feb. Let’s read them in: library(tidyverse) library(here) library(harpIO) read_obs_convert( start_date = 2019021700, end_date = 2019022023, by = &quot;1h&quot;, obs_path = here(&quot;data/vobs&quot;), return_data = TRUE ) ## $synop ## # A tibble: 324,138 x 24 ## validdate SID lat lon elev CCtot D10m S10m T2m Td2m RH2m ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.55e9 1001 70.9 -8.67 9 8 202 8.8 270. 268. 85.3 ## 2 1.55e9 1002 80.1 16.2 8 NA 330 13 248. 246 81.0 ## 3 1.55e9 1003 77 15.5 10 1.04 30 2 252. 245. 52.1 ## 4 1.55e9 1006 78.3 22.8 14 NA 20 3 247. 244. 78.5 ## 5 1.55e9 1007 78.9 11.9 8 NA 350 5 250. NA NA ## 6 1.55e9 1008 78.2 15.5 27 7.04 298 7.6 251. 246 61.9 ## 7 1.55e9 1009 80.7 25.0 5 NA 330 12 244. 241. 76.5 ## 8 1.55e9 1010 69.3 16.1 13 NA 340 11.3 274. 268. 59.9 ## 9 1.55e9 1011 80.1 31.5 9 NA 352 11.4 240. 236. 68.9 ## 10 1.55e9 1013 78.1 13.6 -99 NA NA NA NA NA NA ## # … with 324,128 more rows, and 13 more variables: Q2m &lt;dbl&gt;, Ps &lt;dbl&gt;, ## # Pmsl &lt;dbl&gt;, vis &lt;dbl&gt;, AccPcp3h &lt;dbl&gt;, AccPcp6h &lt;dbl&gt;, AccPcp24h &lt;dbl&gt;, ## # N75 &lt;int&gt;, CClow &lt;int&gt;, Cbase &lt;int&gt;, AccPcp1h &lt;dbl&gt;, Gmax &lt;dbl&gt;, ## # AccPcp12h &lt;dbl&gt; ## ## $temp ## # A tibble: 110,568 x 13 ## validdate SID lat lon elev p Z T RH D S ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.55e9 1001 70.9 -8.67 9 1000 64 269. 83.3 202 3 ## 2 1.55e9 1001 70.9 -8.67 9 925 672 264. 100 205 8 ## 3 1.55e9 1001 70.9 -8.67 9 850 1322 262. 86.4 161 3 ## 4 1.55e9 1001 70.9 -8.67 9 700 2792 254. 32 251 4 ## 5 1.55e9 1001 70.9 -8.67 9 500 5224 240. 8.7 270 18 ## 6 1.55e9 1001 70.9 -8.67 9 400 6762 230. 28.1 275 32 ## 7 1.55e9 1001 70.9 -8.67 9 300 8656 218 38.6 269 43 ## 8 1.55e9 1001 70.9 -8.67 9 250 NA NA NA NA NA ## 9 1.55e9 1001 70.9 -8.67 9 200 11220 218. 2.9 262 28 ## 10 1.55e9 1001 70.9 -8.67 9 150 13048 217. 2.5 257 21 ## # … with 110,558 more rows, and 2 more variables: Q &lt;dbl&gt;, Td &lt;dbl&gt; ## ## $synop_params ## parameter accum_hours units ## 1 CCtot 0 oktas ## 2 D10m 0 degrees ## 3 S10m 0 m/s ## 4 T2m 0 K ## 5 Td2m 0 K ## 6 RH2m 0 percent ## 7 Q2m 0 kg/kg ## 8 Ps 0 hPa ## 9 Pmsl 0 hPa ## 10 vis 0 m ## 11 AccPcp3h 3 kg/m^2 ## 12 AccPcp6h 6 kg/m^2 ## 13 AccPcp24h 24 kg/m^2 ## 14 N75 0 oktas ## 15 CClow 0 oktas ## 16 Cbase 0 m ## 17 AccPcp1h 1 kg/m^2 ## 18 Gmax 0 m/s ## 19 AccPcp12h 12 kg/m^2 ## ## $temp_params ## parameter accum_hours units ## 1 p 0 hPa ## 2 Z 0 m ## 3 T 0 K ## 4 RH 0 percent ## 5 D 0 degrees ## 6 S 0 m/s ## 7 Q 0 kg/kg ## 8 Td 0 K Your turn: Write the observations to SQLite files in the directory data/OBSTABLE Solution read_obs_convert( start_date = 2019021700, end_date = 2019022023, by = &quot;1h&quot;, obs_path = here(&quot;data/vobs&quot;), sqlite_path = here(&quot;data/OBSTABLE&quot;) ) "],
["point-data.html", "Chapter 4 Point Data 4.1 Introduction 4.2 Deterministic data 4.3 Reading in ensemble data 4.4 TO BE ADDED", " Chapter 4 Point Data 4.1 Introduction We now have all of our data prepared for harp and we are ready to work with. That work includes undertsanding how harp data are structured, how to plot forecast data, how to manipulate the data, and verification. We’ll be making use of all of the harp packages in this section, so we can just attach the full set of harp packages (and as always tidyverse and here) library(harp) library(tidyverse) library(here) 4.2 Deterministic data Let’s start with our deterministic data from the AROME Arctic model and member 0 of MEPS. First we load the data into our environment with the function read_point_forecast. As with other functions, we need to tell the function the start and end dates that we want, the names of the models, the parameter, the frequency of the forecasts, the path to the data and whether it is deterministic or ensemble data. s10m &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), fcst_type = &quot;det&quot;, parameter = &quot;S10m&quot;, by = &quot;6h&quot;, file_path = here(&quot;data/FCTABLE/deterministic&quot;) ) s10m ## ● AROME_Arctic_prod ## # A tibble: 17,612 x 10 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 1.55e9 00 0 1.4 NA S10m 1002 m/s ## 3 1.55e9 00 0 24.9 NA S10m 1003 m/s ## 4 1.55e9 00 0 162. NA S10m 1004 m/s ## 5 1.55e9 00 0 -2.7 NA S10m 1006 m/s ## 6 1.55e9 00 0 21.6 NA S10m 1007 m/s ## 7 1.55e9 00 0 67.2 NA S10m 1008 m/s ## 8 1.55e9 00 0 1.2 NA S10m 1009 m/s ## 9 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 10 1.55e9 00 0 20.7 NA S10m 1011 m/s ## # … with 17,602 more rows, and 2 more variables: validdate &lt;int&gt;, ## # AROME_Arctic_prod_det &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 81,804 x 10 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 00 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 00 0 29.1 NA S10m 1014 m/s ## 4 1.55e9 00 0 2.6 NA S10m 1015 m/s ## 5 1.55e9 00 0 152. NA S10m 1018 m/s ## 6 1.55e9 00 0 16.4 NA S10m 1021 m/s ## 7 1.55e9 00 0 39.5 NA S10m 1022 m/s ## 8 1.55e9 00 0 69.1 NA S10m 1023 m/s ## 9 1.55e9 00 0 8.1 NA S10m 1025 m/s ## 10 1.55e9 00 0 5.2 NA S10m 1026 m/s ## # … with 81,794 more rows, and 2 more variables: validdate &lt;int&gt;, ## # MEPS_prod_det &lt;dbl&gt; You will see that data are in two data frames in a list. This list is actually an object of class harp_fcst. Many of the functions in harp only work on harp_fcst objects. Many of the dplyr methods also work on harp_fcst objects. The forecast data are in the last column with the suffix \"_det\". Let’s take a look at some of the things you can do with harp_fcst objects. First expand_data expand_date(s10m, fcdate) ## ● AROME_Arctic_prod ## # A tibble: 17,612 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 1.55e9 00 0 1.4 NA S10m 1002 m/s ## 3 1.55e9 00 0 24.9 NA S10m 1003 m/s ## 4 1.55e9 00 0 162. NA S10m 1004 m/s ## 5 1.55e9 00 0 -2.7 NA S10m 1006 m/s ## 6 1.55e9 00 0 21.6 NA S10m 1007 m/s ## 7 1.55e9 00 0 67.2 NA S10m 1008 m/s ## 8 1.55e9 00 0 1.2 NA S10m 1009 m/s ## 9 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 10 1.55e9 00 0 20.7 NA S10m 1011 m/s ## # … with 17,602 more rows, and 7 more variables: validdate &lt;int&gt;, ## # AROME_Arctic_prod_det &lt;dbl&gt;, fc_year &lt;dbl&gt;, fc_month &lt;dbl&gt;, fc_day &lt;int&gt;, ## # fc_hour &lt;int&gt;, fc_minute &lt;int&gt; ## ## ● MEPS_prod ## # A tibble: 81,804 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 00 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 00 0 29.1 NA S10m 1014 m/s ## 4 1.55e9 00 0 2.6 NA S10m 1015 m/s ## 5 1.55e9 00 0 152. NA S10m 1018 m/s ## 6 1.55e9 00 0 16.4 NA S10m 1021 m/s ## 7 1.55e9 00 0 39.5 NA S10m 1022 m/s ## 8 1.55e9 00 0 69.1 NA S10m 1023 m/s ## 9 1.55e9 00 0 8.1 NA S10m 1025 m/s ## 10 1.55e9 00 0 5.2 NA S10m 1026 m/s ## # … with 81,794 more rows, and 7 more variables: validdate &lt;int&gt;, ## # MEPS_prod_det &lt;dbl&gt;, fc_year &lt;dbl&gt;, fc_month &lt;dbl&gt;, fc_day &lt;int&gt;, ## # fc_hour &lt;int&gt;, fc_minute &lt;int&gt; expand_date(s10m, validdate) ## ● AROME_Arctic_prod ## # A tibble: 17,612 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 1.55e9 00 0 1.4 NA S10m 1002 m/s ## 3 1.55e9 00 0 24.9 NA S10m 1003 m/s ## 4 1.55e9 00 0 162. NA S10m 1004 m/s ## 5 1.55e9 00 0 -2.7 NA S10m 1006 m/s ## 6 1.55e9 00 0 21.6 NA S10m 1007 m/s ## 7 1.55e9 00 0 67.2 NA S10m 1008 m/s ## 8 1.55e9 00 0 1.2 NA S10m 1009 m/s ## 9 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 10 1.55e9 00 0 20.7 NA S10m 1011 m/s ## # … with 17,602 more rows, and 7 more variables: validdate &lt;int&gt;, ## # AROME_Arctic_prod_det &lt;dbl&gt;, valid_year &lt;dbl&gt;, valid_month &lt;dbl&gt;, ## # valid_day &lt;int&gt;, valid_hour &lt;int&gt;, valid_minute &lt;int&gt; ## ## ● MEPS_prod ## # A tibble: 81,804 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 00 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 00 0 29.1 NA S10m 1014 m/s ## 4 1.55e9 00 0 2.6 NA S10m 1015 m/s ## 5 1.55e9 00 0 152. NA S10m 1018 m/s ## 6 1.55e9 00 0 16.4 NA S10m 1021 m/s ## 7 1.55e9 00 0 39.5 NA S10m 1022 m/s ## 8 1.55e9 00 0 69.1 NA S10m 1023 m/s ## 9 1.55e9 00 0 8.1 NA S10m 1025 m/s ## 10 1.55e9 00 0 5.2 NA S10m 1026 m/s ## # … with 81,794 more rows, and 7 more variables: validdate &lt;int&gt;, ## # MEPS_prod_det &lt;dbl&gt;, valid_year &lt;dbl&gt;, valid_month &lt;dbl&gt;, valid_day &lt;int&gt;, ## # valid_hour &lt;int&gt;, valid_minute &lt;int&gt; With the year, month, day, hour and minute now available, it’s easier to filter the data if you’d like. For example, if you just want the forecast wind speed at 15:00 UTC on 17 Feb 2019 for station ID 1010, which we is on Andøya, we could do something like expand_date(s10m, validdate) %&gt;% filter( SID == 1010, valid_year == 2019, valid_month == 2, valid_day == 17, valid_hour == 15 ) ## ● AROME_Arctic_prod ## # A tibble: 3 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 15 3.3 NA S10m 1010 m/s ## 2 1.55e9 06 9 3.3 NA S10m 1010 m/s ## 3 1.55e9 12 3 3.3 NA S10m 1010 m/s ## # … with 7 more variables: validdate &lt;int&gt;, AROME_Arctic_prod_det &lt;dbl&gt;, ## # valid_year &lt;dbl&gt;, valid_month &lt;dbl&gt;, valid_day &lt;int&gt;, valid_hour &lt;int&gt;, ## # valid_minute &lt;int&gt; ## ## ● MEPS_prod ## # A tibble: 3 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 15 4.1 NA S10m 1010 m/s ## 2 1.55e9 06 9 4.1 NA S10m 1010 m/s ## 3 1.55e9 12 3 4.1 NA S10m 1010 m/s ## # … with 7 more variables: validdate &lt;int&gt;, MEPS_prod_det &lt;dbl&gt;, ## # valid_year &lt;dbl&gt;, valid_month &lt;dbl&gt;, valid_day &lt;int&gt;, valid_hour &lt;int&gt;, ## # valid_minute &lt;int&gt; Equally, we could just get the data for one forecast expand_date(s10m, fcdate) %&gt;% filter( SID == 1010, fc_year == 2019, fc_month == 2, fc_day == 17, fc_hour == 12 ) %&gt;% mutate(validdate = unix2datetime(validdate)) ## ● AROME_Arctic_prod ## # A tibble: 17 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 12 0 3.3 NA S10m 1010 m/s ## 2 1.55e9 12 3 3.3 NA S10m 1010 m/s ## 3 1.55e9 12 6 3.3 NA S10m 1010 m/s ## 4 1.55e9 12 9 3.3 NA S10m 1010 m/s ## 5 1.55e9 12 12 3.3 NA S10m 1010 m/s ## 6 1.55e9 12 15 3.3 NA S10m 1010 m/s ## 7 1.55e9 12 18 3.3 NA S10m 1010 m/s ## 8 1.55e9 12 21 3.3 NA S10m 1010 m/s ## 9 1.55e9 12 24 3.3 NA S10m 1010 m/s ## 10 1.55e9 12 27 3.3 NA S10m 1010 m/s ## 11 1.55e9 12 30 3.3 NA S10m 1010 m/s ## 12 1.55e9 12 33 3.3 NA S10m 1010 m/s ## 13 1.55e9 12 36 3.3 NA S10m 1010 m/s ## 14 1.55e9 12 39 3.3 NA S10m 1010 m/s ## 15 1.55e9 12 42 3.3 NA S10m 1010 m/s ## 16 1.55e9 12 45 3.3 NA S10m 1010 m/s ## 17 1.55e9 12 48 3.3 NA S10m 1010 m/s ## # … with 7 more variables: validdate &lt;dttm&gt;, AROME_Arctic_prod_det &lt;dbl&gt;, ## # fc_year &lt;dbl&gt;, fc_month &lt;dbl&gt;, fc_day &lt;int&gt;, fc_hour &lt;int&gt;, fc_minute &lt;int&gt; ## ## ● MEPS_prod ## # A tibble: 17 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 12 0 4.1 NA S10m 1010 m/s ## 2 1.55e9 12 3 4.1 NA S10m 1010 m/s ## 3 1.55e9 12 6 4.1 NA S10m 1010 m/s ## 4 1.55e9 12 9 4.1 NA S10m 1010 m/s ## 5 1.55e9 12 12 4.1 NA S10m 1010 m/s ## 6 1.55e9 12 15 4.1 NA S10m 1010 m/s ## 7 1.55e9 12 18 4.1 NA S10m 1010 m/s ## 8 1.55e9 12 21 4.1 NA S10m 1010 m/s ## 9 1.55e9 12 24 4.1 NA S10m 1010 m/s ## 10 1.55e9 12 27 4.1 NA S10m 1010 m/s ## 11 1.55e9 12 30 4.1 NA S10m 1010 m/s ## 12 1.55e9 12 33 4.1 NA S10m 1010 m/s ## 13 1.55e9 12 36 4.1 NA S10m 1010 m/s ## 14 1.55e9 12 39 4.1 NA S10m 1010 m/s ## 15 1.55e9 12 42 4.1 NA S10m 1010 m/s ## 16 1.55e9 12 45 4.1 NA S10m 1010 m/s ## 17 1.55e9 12 48 4.1 NA S10m 1010 m/s ## # … with 7 more variables: validdate &lt;dttm&gt;, MEPS_prod_det &lt;dbl&gt;, ## # fc_year &lt;dbl&gt;, fc_month &lt;dbl&gt;, fc_day &lt;int&gt;, fc_hour &lt;int&gt;, fc_minute &lt;int&gt; harp doesn’t yet include any functions for plotting deterministic forecasts, but we can easily make the data into a single data frame using bind_fcst. bind_rows( AROME_Arctic_prod = rename(s10m$AROME_Arctic_prod, forecast = AROME_Arctic_prod_det), MEPS_prod = rename(s10m$MEPS_prod, forecast = MEPS_prod_det), .id = &quot;mname&quot; ) ## # A tibble: 99,416 x 11 ## mname fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 AROM… 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 AROM… 1.55e9 00 0 1.4 NA S10m 1002 m/s ## 3 AROM… 1.55e9 00 0 24.9 NA S10m 1003 m/s ## 4 AROM… 1.55e9 00 0 162. NA S10m 1004 m/s ## 5 AROM… 1.55e9 00 0 -2.7 NA S10m 1006 m/s ## 6 AROM… 1.55e9 00 0 21.6 NA S10m 1007 m/s ## 7 AROM… 1.55e9 00 0 67.2 NA S10m 1008 m/s ## 8 AROM… 1.55e9 00 0 1.2 NA S10m 1009 m/s ## 9 AROM… 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 10 AROM… 1.55e9 00 0 20.7 NA S10m 1011 m/s ## # … with 99,406 more rows, and 2 more variables: validdate &lt;int&gt;, ## # forecast &lt;dbl&gt; You will also find in R directory a function, bind_fcst that does this binding for you - this function will be part of harp soon. bind_fcst(s10m) ## # A tibble: 99,416 x 11 ## mname fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 AROM… 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 AROM… 1.55e9 00 0 1.4 NA S10m 1002 m/s ## 3 AROM… 1.55e9 00 0 24.9 NA S10m 1003 m/s ## 4 AROM… 1.55e9 00 0 162. NA S10m 1004 m/s ## 5 AROM… 1.55e9 00 0 -2.7 NA S10m 1006 m/s ## 6 AROM… 1.55e9 00 0 21.6 NA S10m 1007 m/s ## 7 AROM… 1.55e9 00 0 67.2 NA S10m 1008 m/s ## 8 AROM… 1.55e9 00 0 1.2 NA S10m 1009 m/s ## 9 AROM… 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 10 AROM… 1.55e9 00 0 20.7 NA S10m 1011 m/s ## # … with 99,406 more rows, and 2 more variables: validdate &lt;int&gt;, ## # forecast &lt;dbl&gt; Your turn: Plot the the forecast wind speed at REIPA for both AROME_Arctic_prod and MEPS_prod for each fcst_cycle as a function of validdate. [You can get the SID for REIPA from the built in list of stations which is in the variable station_list] Solution: reipa_sid &lt;- filter(station_list, name == &quot;REIPA&quot;) %&gt;% pull(SID) bind_fcst(s10m) %&gt;% filter(SID == reipa_sid) %&gt;% mutate(validdate = unix2datetime(validdate)) %&gt;% ggplot(aes(validdate, forecast, colour = fcst_cycle)) + geom_line() + facet_wrap(vars(mname), ncol = 1) + scale_x_datetime( breaks = lubridate::ymd_hm(seq_dates(2019021706, 2019021918, &quot;12h&quot;)) ) + labs( x = &quot;Date-time&quot;, y = bquote(&quot;Wind speed [ms&quot;^-1*&quot;]&quot;), colour = &quot;Forecast cycle&quot;, title = &quot;Forecast Wind Speed at REIPA&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) bind_fcst(s10m) %&gt;% filter(SID == reipa_sid) %&gt;% mutate(validdate = unix2datetime(validdate)) %&gt;% ggplot(aes(validdate, forecast, colour = mname)) + geom_line() + facet_wrap(vars(fcst_cycle), ncol = 1) + scale_x_datetime( breaks = lubridate::ymd_hm(seq_dates(2019021706, 2019021918, &quot;12h&quot;)) ) + labs( x = &quot;Date-time&quot;, y = bquote(&quot;Wind speed [ms&quot;^-1*&quot;]&quot;), colour = &quot;&quot;, title = &quot;Forecast Wind Speed at REIPA&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Now let’s read in some observations. To make sure we get the correct dates for the observations we can get the first and last validdates from the forecast. We could also extract the station IDs from the forecasts - which we can do with a little bit of “functional programming”. station_ids &lt;- map(s10m, pull, SID) %&gt;% reduce(union) obs &lt;- read_point_obs( start_date = first_validdate(s10m), end_date = last_validdate(s10m), stations = station_ids, parameter = &quot;S10m&quot;, obs_path = here(&quot;data/OBSTABLE&quot;) ) Now that we have the observations we can join them to our forecast data using the function join_to_fcst. This will take each of the data frames in our forecast list and perform an inner join with the observations - that means that only rows that are common to both data frames are kept. s10m &lt;- join_to_fcst(s10m, obs) We have now added an observations column to each of the forecasts, so we could also include the observations in our plots. Your turn: Add observations as a geom_point to your plots. [hint: you can map an aesthetic to an individual geom] Solution: bind_fcst(s10m) %&gt;% filter(SID == reipa_sid) %&gt;% mutate(validdate = unix2datetime(validdate)) %&gt;% ggplot(aes(validdate, forecast, colour = mname)) + geom_line() + geom_point(aes(y = S10m, shape = &quot;Observation&quot;), colour = &quot;blue&quot;) + scale_shape_manual(values = 21) + facet_wrap(vars(fcst_cycle), ncol = 1) + scale_x_datetime( breaks = lubridate::ymd_hm(seq_dates(2019021706, 2019021918, &quot;12h&quot;)) ) + labs( x = &quot;Date-time&quot;, y = bquote(&quot;Wind speed [ms&quot;^-1*&quot;]&quot;), colour = &quot;&quot;, title = &quot;Forecast Wind Speed at REIPA&quot;, shape = &quot;&quot; ) + theme_bw() + theme(legend.position = &quot;bottom&quot;) 4.2.1 Verifying deterministic forecasts Now that we have both forecasts and observations we can verify the forecasts. This is very simple in harp - we just run the function det_verify giving that data and telling it which column has the observations: det_verify(s10m, S10m) ## $det_summary_scores ## # A tibble: 34 x 7 ## mname leadtime num_cases bias rmse mae stde ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AROME_Arctic_prod 0 629 0.137 2.57 1.80 2.57 ## 2 AROME_Arctic_prod 3 679 0.0580 2.34 1.60 2.34 ## 3 AROME_Arctic_prod 6 622 0.0390 2.38 1.65 2.38 ## 4 AROME_Arctic_prod 9 679 0.0942 2.08 1.42 2.08 ## 5 AROME_Arctic_prod 12 628 0.169 2.24 1.53 2.24 ## 6 AROME_Arctic_prod 15 675 0.295 2.17 1.53 2.15 ## 7 AROME_Arctic_prod 18 630 0.319 2.22 1.53 2.20 ## 8 AROME_Arctic_prod 21 676 0.480 2.23 1.61 2.17 ## 9 AROME_Arctic_prod 24 628 0.388 2.34 1.61 2.31 ## 10 AROME_Arctic_prod 27 674 0.538 2.39 1.69 2.33 ## # … with 24 more rows ## ## $det_threshold_scores ## data frame with 0 columns and 0 rows ## ## attr(,&quot;parameter&quot;) ## [1] &quot;S10m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;2019021700&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;2019021718&quot; ## attr(,&quot;num_stations&quot;) ## [1] 828 You will see that the output is a list of two data frames - one fore summary scores and one for threshold scores. The threshold scores data frame contains only missing data as we didn’t give the function any thresholds to verify for. However, that is easily done: verif_s10m &lt;- det_verify(s10m, S10m, thresholds = seq(2.5, 12.5, 2.5)) verif_s10m ## $det_summary_scores ## # A tibble: 34 x 7 ## mname leadtime num_cases bias rmse mae stde ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AROME_Arctic_prod 0 629 0.137 2.57 1.80 2.57 ## 2 AROME_Arctic_prod 3 679 0.0580 2.34 1.60 2.34 ## 3 AROME_Arctic_prod 6 622 0.0390 2.38 1.65 2.38 ## 4 AROME_Arctic_prod 9 679 0.0942 2.08 1.42 2.08 ## 5 AROME_Arctic_prod 12 628 0.169 2.24 1.53 2.24 ## 6 AROME_Arctic_prod 15 675 0.295 2.17 1.53 2.15 ## 7 AROME_Arctic_prod 18 630 0.319 2.22 1.53 2.20 ## 8 AROME_Arctic_prod 21 676 0.480 2.23 1.61 2.17 ## 9 AROME_Arctic_prod 24 628 0.388 2.34 1.61 2.31 ## 10 AROME_Arctic_prod 27 674 0.538 2.39 1.69 2.33 ## # … with 24 more rows ## ## $det_threshold_scores ## # A tibble: 170 x 40 ## mname leadtime threshold num_cases_for_t… num_cases_for_t… num_cases_for_t… ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AROM… 0 2.5 461 418 421 ## 2 AROM… 3 2.5 482 418 436 ## 3 AROM… 6 2.5 410 348 358 ## 4 AROM… 9 2.5 449 365 403 ## 5 AROM… 12 2.5 410 328 363 ## 6 AROM… 15 2.5 459 358 422 ## 7 AROM… 18 2.5 437 338 389 ## 8 AROM… 21 2.5 497 374 466 ## 9 AROM… 24 2.5 452 348 413 ## 10 AROM… 27 2.5 496 374 471 ## # … with 160 more rows, and 34 more variables: cont_tab &lt;list&gt;, ## # threat_score &lt;dbl&gt;, hit_rate &lt;dbl&gt;, miss_rate &lt;dbl&gt;, ## # false_alarm_rate &lt;dbl&gt;, false_alarm_ratio &lt;dbl&gt;, heidke_skill_score &lt;dbl&gt;, ## # pierce_skill_score &lt;dbl&gt;, kuiper_skill_score &lt;dbl&gt;, percent_correct &lt;dbl&gt;, ## # frequency_bias &lt;dbl&gt;, equitable_threat_score &lt;dbl&gt;, odds_ratio &lt;dbl&gt;, ## # log_odds_ratio &lt;dbl&gt;, odds_ratio_skill_score &lt;dbl&gt;, ## # extreme_dependency_score &lt;dbl&gt;, symmetric_eds &lt;dbl&gt;, ## # extreme_dependency_index &lt;dbl&gt;, symmetric_edi &lt;dbl&gt;, ## # threat_score_std_error &lt;dbl&gt;, hit_rate_std_error &lt;dbl&gt;, ## # false_alarm_rate_std_error &lt;dbl&gt;, false_alarm_ratio_std_error &lt;dbl&gt;, ## # heidke_skill_score_std_error &lt;dbl&gt;, pierce_skill_score_std_error &lt;dbl&gt;, ## # percent_correct_std_error &lt;dbl&gt;, equitable_threat_score_std_error &lt;dbl&gt;, ## # log_odds_ratio_std_error &lt;dbl&gt;, log_odds_ratio_degrees_of_freedom &lt;dbl&gt;, ## # odds_ratio_skill_score_std_error &lt;dbl&gt;, ## # extreme_dependency_score_std_error &lt;dbl&gt;, symmetric_eds_std_error &lt;dbl&gt;, ## # extreme_dependency_index_std_error &lt;dbl&gt;, symmetric_edi_std_error &lt;dbl&gt; ## ## attr(,&quot;parameter&quot;) ## [1] &quot;S10m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;2019021700&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;2019021718&quot; ## attr(,&quot;num_stations&quot;) ## [1] 828 As you can see, a very large number of scores are computed. harp has a function for plotting point verification scores, plot_point_verif. For summary scores, you just need to give it the verification data and tell it which score you’d like to plot, as well as tell the function that it is deterministic verification data. plot_point_verif(verif_s10m, bias, verif_type = &quot;det&quot;) One thing you’ll immediately notice is that the number of cases for MEPS_prod is much larger than for AROME_Arctic_prod. This means that in this verification we are not making a fair comparison - we should actually only be verifying the dates and locations that are common to both systems. harp provides the function common_cases() to do just that! common_cases(s10m) ## ● AROME_Arctic_prod ## # A tibble: 10,170 x 14 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 63.9 NA S10m 1001 m/s ## 2 1.55e9 00 0 3.3 NA S10m 1010 m/s ## 3 1.55e9 00 0 0.5 NA S10m 1015 m/s ## 4 1.55e9 00 0 169. NA S10m 1018 m/s ## 5 1.55e9 00 0 73.5 NA S10m 1023 m/s ## 6 1.55e9 00 0 22.2 NA S10m 1025 m/s ## 7 1.55e9 00 0 14.4 NA S10m 1026 m/s ## 8 1.55e9 00 0 -2.6 NA S10m 1033 m/s ## 9 1.55e9 00 0 125. NA S10m 1036 m/s ## 10 1.55e9 00 0 160. NA S10m 1037 m/s ## # … with 10,160 more rows, and 6 more variables: validdate &lt;int&gt;, ## # AROME_Arctic_prod_det &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, S10m &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 10,170 x 14 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 00 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 00 0 2.6 NA S10m 1015 m/s ## 4 1.55e9 00 0 152. NA S10m 1018 m/s ## 5 1.55e9 00 0 69.1 NA S10m 1023 m/s ## 6 1.55e9 00 0 8.1 NA S10m 1025 m/s ## 7 1.55e9 00 0 5.2 NA S10m 1026 m/s ## 8 1.55e9 00 0 28.5 NA S10m 1033 m/s ## 9 1.55e9 00 0 129 NA S10m 1036 m/s ## 10 1.55e9 00 0 94.5 NA S10m 1037 m/s ## # … with 10,160 more rows, and 6 more variables: validdate &lt;int&gt;, ## # MEPS_prod_det &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, S10m &lt;dbl&gt; s10m &lt;- common_cases(s10m) You will see that now both AROME_Arctic_prod and MEPS_prod have the exact same number of rows in their data. Now if we run the verfication again and plot the same score we will see that both forecasting systems have the same number of cases. verif_s10m &lt;- det_verify(s10m, S10m, thresholds = seq(2.5, 12.5, 2.5)) plot_point_verif(verif_s10m, bias, verif_type = &quot;det&quot;) Your turn: Try plotting one of the threshold scores (frequency_bias, for example). Try to figure out wht the plot looks so weird Solution: plot_point_verif(verif_s10m, frequency_bias, verif_type = &quot;det&quot;) The plot looks weird becasue it’s attempting to plot scores for all thresholds at the same time. When we have more than one threshold, we need to tell plot_point_verif what to do - there are 2 options - to facet or to filter, with the arguments facet_by or filter_by. These arguments work in pretty much the same way as facet_wrap. For example if we wanted the facet the scores by threshold, we would do plot_point_verif( verif_s10m, frequency_bias, verif_type = &quot;det&quot;, facet_by = vars(threshold) ) Or, if we just wanted the plot for a threshold of 7.5 ms-1, we would use filter_by plot_point_verif( verif_s10m, frequency_bias, verif_type = &quot;det&quot;, filter_by = vars(threshold == 7.5) ) 4.2.2 Verification by group The default behaviour of harp verification functions is to compute the verification metrics for each lead time. However, you can also compute the scores for any groups of data, much in the same way as group_by enables you to do. In this case groups are specified in the groupings argument to the verification function, and unlike group_by the column names you wish to use for grouping variables must be quoted (this may change in the future for consistency with group_by) and if there are more than one in a charcater vector. We could for example compute scores for each valid time: (verif_s10m &lt;- det_verify(s10m, S10m, groupings = &quot;validdate&quot;)) ## $det_summary_scores ## # A tibble: 46 x 7 ## mname validdate num_cases bias rmse mae stde ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AROME_Arctic_prod 1550361600 147 0.276 3.04 2.28 3.04 ## 2 AROME_Arctic_prod 1550372400 155 0.121 2.90 2.11 2.91 ## 3 AROME_Arctic_prod 1550383200 270 0.0203 2.68 1.81 2.68 ## 4 AROME_Arctic_prod 1550394000 312 -0.295 1.95 1.35 1.93 ## 5 AROME_Arctic_prod 1550404800 459 -0.235 2.07 1.44 2.06 ## 6 AROME_Arctic_prod 1550415600 468 -0.112 1.95 1.26 1.95 ## 7 AROME_Arctic_prod 1550426400 572 -0.0128 1.84 1.30 1.84 ## 8 AROME_Arctic_prod 1550437200 628 0.192 2.05 1.39 2.05 ## 9 AROME_Arctic_prod 1550448000 560 -0.0470 2.25 1.53 2.25 ## 10 AROME_Arctic_prod 1550458800 620 0.126 2.04 1.45 2.03 ## # … with 36 more rows ## ## $det_threshold_scores ## data frame with 0 columns and 0 rows ## ## attr(,&quot;parameter&quot;) ## [1] &quot;S10m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;2019021700&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;2019021718&quot; ## attr(,&quot;num_stations&quot;) ## [1] 167 To plot the score, we then need to tell plot_point_verif to use validdate as the x-axis (the default is lead time) plot_point_verif(verif_s10m, bias, x_axis = validdate) If you want to change the date into a readable format, you can use the mutate_list function. plot_point_verif( mutate_list(verif_s10m, date_time = unix2datetime(validdate)), bias, x_axis = date_time ) Your turn: Compute verification scores for stations with elevations \\(\\geqslant 300 m\\) and those \\(\\lt 300 m\\). for each lead time and thresholds of 2.5 - 12.5 ms-1 every 2.5 ms-1. To classify the station heights, you can use stations &lt;- mutate( station_list, station_height = cut(elev, breaks = c(-Inf, 300, Inf), labels = c(&quot;lt_300m&quot;, &quot;ge_300m&quot;)) ) s10m &lt;- join_to_fcst(s10m, stations, force_join = TRUE) Plot the equitable threat score for each threshold and station height group. Solutions Compute verification scores for stations with elevations \\(\\geqslant 300 m\\) and those \\(\\lt 300 m\\). for each lead time and thresholds of 2.5 - 12.5 ms-1 every 2.5 ms-1. stations &lt;- mutate( station_list, station_height = cut(elev, breaks = c(-Inf, 300, Inf), labels = c(&quot;&gt; 300m&quot;, &quot;&gt;= 300m&quot;)) ) (verif_s10m &lt;- join_to_fcst(s10m, stations, force_join = TRUE) %&gt;% det_verify(S10m, groupings = c(&quot;leadtime&quot;, &quot;station_height&quot;), thresholds = seq(2.5, 12.5, 2.5))) ## $det_summary_scores ## # A tibble: 0 x 8 ## # … with 8 variables: mname &lt;chr&gt;, leadtime &lt;int&gt;, station_height &lt;fct&gt;, ## # num_cases &lt;int&gt;, bias &lt;dbl&gt;, rmse &lt;dbl&gt;, mae &lt;dbl&gt;, stde &lt;dbl&gt; ## ## $det_threshold_scores ## # A tibble: 0 x 41 ## # … with 41 variables: mname &lt;chr&gt;, leadtime &lt;int&gt;, station_height &lt;fct&gt;, ## # threshold &lt;dbl&gt;, num_cases_for_threshold_total &lt;dbl&gt;, ## # num_cases_for_threshold_observed &lt;dbl&gt;, ## # num_cases_for_threshold_forecast &lt;dbl&gt;, cont_tab &lt;list&gt;, ## # threat_score &lt;dbl&gt;, hit_rate &lt;dbl&gt;, miss_rate &lt;dbl&gt;, ## # false_alarm_rate &lt;dbl&gt;, false_alarm_ratio &lt;dbl&gt;, heidke_skill_score &lt;dbl&gt;, ## # pierce_skill_score &lt;dbl&gt;, kuiper_skill_score &lt;dbl&gt;, percent_correct &lt;dbl&gt;, ## # frequency_bias &lt;dbl&gt;, equitable_threat_score &lt;dbl&gt;, odds_ratio &lt;dbl&gt;, ## # log_odds_ratio &lt;dbl&gt;, odds_ratio_skill_score &lt;dbl&gt;, ## # extreme_dependency_score &lt;dbl&gt;, symmetric_eds &lt;dbl&gt;, ## # extreme_dependency_index &lt;dbl&gt;, symmetric_edi &lt;dbl&gt;, ## # threat_score_std_error &lt;dbl&gt;, hit_rate_std_error &lt;dbl&gt;, ## # false_alarm_rate_std_error &lt;dbl&gt;, false_alarm_ratio_std_error &lt;dbl&gt;, ## # heidke_skill_score_std_error &lt;dbl&gt;, pierce_skill_score_std_error &lt;dbl&gt;, ## # percent_correct_std_error &lt;dbl&gt;, equitable_threat_score_std_error &lt;dbl&gt;, ## # log_odds_ratio_std_error &lt;dbl&gt;, log_odds_ratio_degrees_of_freedom &lt;dbl&gt;, ## # odds_ratio_skill_score_std_error &lt;dbl&gt;, ## # extreme_dependency_score_std_error &lt;dbl&gt;, symmetric_eds_std_error &lt;dbl&gt;, ## # extreme_dependency_index_std_error &lt;dbl&gt;, symmetric_edi_std_error &lt;dbl&gt; ## ## attr(,&quot;parameter&quot;) ## [1] &quot;S10m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;NANANANA&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;NANANANA&quot; ## attr(,&quot;num_stations&quot;) ## [1] 0 Plot the equitable threat score for each threshold and station height group. plot_point_verif( mutate_list( verif_s10m, threshold = paste(&quot;Wind speed &gt;=&quot;, threshold, &quot;m/s&quot;), station_height = paste(&quot;Station altitude&quot;, station_height) ), equitable_threat_score, facet_by = vars(fct_inorder(threshold), station_height), num_facet_cols = 2 ) ## NULL Bearing in mind the plot_point_verif uses ggplot, you could also do the faceting yourself and have more control - for example, you coud use facet_grid plot_point_verif( mutate_list( verif_s10m, threshold = paste(&quot;S10m &gt;=&quot;, threshold, &quot;m/s&quot;), station_height = paste(&quot;Station altitude&quot;, station_height) ), equitable_threat_score ) + facet_grid(cols = vars(station_height), rows = vars(fct_inorder(threshold))) ## NULL 4.2.3 Vertical profiles When we converted our data to sqlite, we also converted some upper air data for temperature and dew point temperature. To read in upper air data, we need to tell read_point_forecast and read_point_obs what the vertical coordinate is. (t_upper &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), fcst_type = &quot;det&quot;, parameter = &quot;T&quot;, file_path = here(&quot;data/FCTABLE/deterministic&quot;), vertical_coordinate = &quot;pressure&quot; )) ## ● AROME_Arctic_prod ## # A tibble: 6,967 x 10 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 50.6 50 T 1001 K ## 2 1.55e9 00 0 50.6 100 T 1001 K ## 3 1.55e9 00 0 50.6 150 T 1001 K ## 4 1.55e9 00 0 50.6 200 T 1001 K ## 5 1.55e9 00 0 50.6 250 T 1001 K ## 6 1.55e9 00 0 50.6 300 T 1001 K ## 7 1.55e9 00 0 50.6 400 T 1001 K ## 8 1.55e9 00 0 50.6 500 T 1001 K ## 9 1.55e9 00 0 50.6 700 T 1001 K ## 10 1.55e9 00 0 50.6 850 T 1001 K ## # … with 6,957 more rows, and 2 more variables: validdate &lt;int&gt;, ## # AROME_Arctic_prod_det &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 29,956 x 10 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 55.6 50 T 1001 K ## 2 1.55e9 00 0 55.6 100 T 1001 K ## 3 1.55e9 00 0 55.6 150 T 1001 K ## 4 1.55e9 00 0 55.6 200 T 1001 K ## 5 1.55e9 00 0 55.6 250 T 1001 K ## 6 1.55e9 00 0 55.6 300 T 1001 K ## 7 1.55e9 00 0 55.6 400 T 1001 K ## 8 1.55e9 00 0 55.6 500 T 1001 K ## 9 1.55e9 00 0 55.6 700 T 1001 K ## 10 1.55e9 00 0 55.6 850 T 1001 K ## # … with 29,946 more rows, and 2 more variables: validdate &lt;int&gt;, ## # MEPS_prod_det &lt;dbl&gt; td_upper &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;AROME_Arctic_prod&quot;, &quot;MEPS_prod&quot;), fcst_type = &quot;det&quot;, parameter = &quot;Td&quot;, file_path = here(&quot;data/FCTABLE/deterministic&quot;), vertical_coordinate = &quot;pressure&quot; ) harp has a function for plotting vertical profile, that allows you compare the profiles from different models - but only for the same parameter. plot_vertical_profile( t_upper, SID = 22113, fcdate = 2019021712, lead_time = 24 ) You can also plot the profiles on skew-t / log P grid by setting skew_t = TRUE. However, it should be noted that temperatures need to be in °C rather than Kelvin. We can convert the units by using scale_point_forecast. scale_point_forecast(t_upper, -273.15, new_units = &quot;degC&quot;) %&gt;% plot_vertical_profile( SID = 22113, fcdate = 2019021712, lead_time = 24, skew_t = TRUE ) Your turn: You can specify more than 1 station, date, and / or lead time in plot_vertical_profile. Experiment with making some multi panel plots. Can you figure out how you would add the dew point temperature to the plots? Solution Experiment with making some multi panel plots. plot_vertical_profile( t_upper, SID = 22113, fcdate = 2019021700, lead_time = seq(0, 30, 6), facet_by = vars(leadtime) ) Can you figure out how you would add the dew point temperature to the plots? plot_vertical_profile( t_upper, SID = 22113, fcdate = 2019021712, lead_time = 24 ) + geom_path( data = filter( bind_fcst(td_upper), SID == 22113, fcdate == str_datetime_to_unixtime(2019021712), leadtime == 24 ), aes(linetype = &quot;Dew Point&quot;) ) + scale_linetype_manual(&quot;&quot;, values = 2) 4.3 Reading in ensemble data Now we have seen harp in action with deterministic data, let’s move on to ensemble data. harp’s origins are in verification of ensemble data, so the functionality here is a little more mature. Many of the same functions that were introduced in the deterministic section can be used for ensemble data as well. There are also functions that currently only work with ensemble data and are in the process of being upgraded to handle deterministic forecasts as well. We can use the same read_point_forecast function to read in ensemble data - we just need to tell it that we are reading ensemble data by setting fcst_type = \"eps\". library(tidyverse) library(here) library(harp) (t2m &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;MEPS_prod&quot;, fcst_type = &quot;EPS&quot;, parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), by = &quot;6h&quot;, file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot; )) ## ● MEPS_prod ## # A tibble: 22,980 x 19 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 29.1 NA T2m 1014 K ## 4 1.55e9 00 0 2.6 NA T2m 1015 K ## 5 1.55e9 00 0 152. NA T2m 1018 K ## 6 1.55e9 00 0 69.1 NA T2m 1023 K ## 7 1.55e9 00 0 8.1 NA T2m 1025 K ## 8 1.55e9 00 0 5.2 NA T2m 1026 K ## 9 1.55e9 00 0 5.2 NA T2m 1027 K ## 10 1.55e9 00 0 28.5 NA T2m 1033 K ## # … with 22,970 more rows, and 11 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt; You will see that each ensemble member gets its own column in the data frame. This isn’t really tidy data - for the data to be tidy, the ensemble member would be a variable as well. However, when ensembles become large the slightly untidy format is advantageous - firstly, many of the verification functions we use expect the ensemble data as a matrix and continual pivoting slows these function down a lot and secondly, this format takes up less space in memory. We can, however, convert the data to a tidy format with the function gather_members() gather_members(t2m) ## ● MEPS_prod ## # A tibble: 229,800 x 12 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 29.1 NA T2m 1014 K ## 4 1.55e9 00 0 2.6 NA T2m 1015 K ## 5 1.55e9 00 0 152. NA T2m 1018 K ## 6 1.55e9 00 0 69.1 NA T2m 1023 K ## 7 1.55e9 00 0 8.1 NA T2m 1025 K ## 8 1.55e9 00 0 5.2 NA T2m 1026 K ## 9 1.55e9 00 0 5.2 NA T2m 1027 K ## 10 1.55e9 00 0 28.5 NA T2m 1033 K ## # … with 229,790 more rows, and 4 more variables: validdate &lt;int&gt;, ## # member &lt;chr&gt;, forecast &lt;dbl&gt;, sub_model &lt;chr&gt; With the members gathered together we can use bind_fcst in order to make basic plots from the data using ggplot. bind_fcst(t2m) %&gt;% filter(SID == 1492, fcdate == str_datetime_to_unixtime(2019021712)) %&gt;% ggplot(aes(x = unix2datetime(validdate), y = forecast, colour = member)) + geom_line() bind_fcst(t2m) %&gt;% mutate(leadtime = fct_inorder(paste0(&quot;Lead time: &quot;, leadtime, &quot;h&quot;))) %&gt;% ggplot(aes(forecast, colour = leadtime, fill = leadtime)) + geom_density(alpha = 0.5) + facet_wrap(vars(leadtime)) + theme(legend.position = &quot;none&quot;) There are also a number of different ways you can plot time series of ensemble forecasts for a station. This can be done with the function plot_station_eps. You give it the harp_fcst object and tell it what station and forecast start time you want the plot for, and optionally what sort of plot you want. plot_station_eps(t2m, 1492, 2019021700) plot_station_eps(t2m, 1492, 2019021700, type = &quot;boxplot&quot;) plot_station_eps(t2m, 1492, 2019021700, type = &quot;spaghetti&quot;) plot_station_eps(t2m, 1492, 2019021700, type = &quot;ridge&quot;) There are other types of plot that are more suited to truncated distributions such as those that are bound at 0 for wind speed and precipitation. When we converted the precipitation data to SQLite, we do not take any accout of accumulation times - the data are as they are output from in the model as accumulation since the model start time. We can do the accumulation when we read in the data using read_point_forecast by prefixing the parameter name with “Acc” for accumulated and suffixing it with the accumulation time in hours, e.g. “6h”. Your turn: Read in 3h accumulated precipitatino from MEPS. Use show_harp_parameters() if you need some guidance. Find the time and station with the heighest forecast precipitation, and which forecast it s for. Solution: Read in 3h accumulated precipitatino from MEPS. Use show_harp_parameters() if you need some guidance. (precip_3h &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;MEPS_prod&quot;, fcst_type = &quot;EPS&quot;, parameter = &quot;AccPcp3h&quot;, lead_time = seq(0, 12, 3), by = &quot;6h&quot;, file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot; )) ## ● MEPS_prod ## # A tibble: 19,248 x 18 ## fcdate fcst_cycle leadtime model_elevation parameter SID units validdate ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1.55e9 00 3 -24.7 Pcp 1320 kg/m… 1.55e9 ## 2 1.55e9 00 3 -12.1 Pcp 1043 kg/m… 1.55e9 ## 3 1.55e9 00 3 -10.7 Pcp 1193 kg/m… 1.55e9 ## 4 1.55e9 00 3 -7 Pcp 1117 kg/m… 1.55e9 ## 5 1.55e9 00 3 -6.7 Pcp 1112 kg/m… 1.55e9 ## 6 1.55e9 00 3 -5.8 Pcp 1161 kg/m… 1.55e9 ## 7 1.55e9 00 3 -4.7 Pcp 1098 kg/m… 1.55e9 ## 8 1.55e9 00 3 -4.7 Pcp 1162 kg/m… 1.55e9 ## 9 1.55e9 00 3 -3.4 Pcp 3007 kg/m… 1.55e9 ## 10 1.55e9 00 3 -2.7 Pcp 1476 kg/m… 1.55e9 ## # … with 19,238 more rows, and 10 more variables: MEPS_prod_mbr000 &lt;dbl&gt;, ## # MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, MEPS_prod_mbr003 &lt;dbl&gt;, ## # MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, MEPS_prod_mbr006 &lt;dbl&gt;, ## # MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, MEPS_prod_mbr009 &lt;dbl&gt; Find the time and station with the heighest forecast precipitation bind_fcst(precip_3h) %&gt;% group_by(SID, fcdate, validdate) %&gt;% summarise(max_precip = max(forecast)) %&gt;% arrange(desc(max_precip)) %&gt;% ungroup() %&gt;% mutate(fcdate = unix2datetime(fcdate), validdate = unix2datetime(validdate)) ## # A tibble: 19,248 x 4 ## SID fcdate validdate max_precip ## &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1106 2019-02-17 06:00:00 2019-02-17 09:00:00 10.3 ## 2 1107 2019-02-17 06:00:00 2019-02-17 09:00:00 10.3 ## 3 1121 2019-02-17 00:00:00 2019-02-17 03:00:00 7.46 ## 4 22113 2019-02-17 00:00:00 2019-02-17 06:00:00 6.16 ## 5 1611 2019-02-17 00:00:00 2019-02-17 03:00:00 6.13 ## 6 1015 2019-02-17 12:00:00 2019-02-17 21:00:00 5.77 ## 7 1201 2019-02-17 12:00:00 2019-02-17 21:00:00 5.65 ## 8 1611 2019-02-17 18:00:00 2019-02-18 06:00:00 5.64 ## 9 1014 2019-02-17 06:00:00 2019-02-17 18:00:00 5.28 ## 10 1122 2019-02-17 00:00:00 2019-02-17 03:00:00 5.27 ## # … with 19,238 more rows Let’s plot a time series for that forecast. plot_station_eps(precip_3h, 1106, 2019021706, type = &quot;boxplot&quot;) + scale_x_continuous(breaks = seq(0, 12, 3)) plot_station_eps(precip_3h, 1106, 2019021706, type = &quot;stacked_prob&quot;) + scale_x_continuous(breaks = seq(0, 12, 3)) Your turn: Observations are read in the same way as we did for deterministic forecasts. Read in the observations for 2m temperature and 3h accumulated precipitation and join to the forecasts. (t2m &lt;- join_to_fcst( t2m, read_point_obs( first_validdate(t2m), last_validdate(t2m), parameter = &quot;T2m&quot;, obs_path = here(&quot;data/OBSTABLE&quot;) ) )) ## ● MEPS_prod ## # A tibble: 15,963 x 23 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 2.6 NA T2m 1015 K ## 4 1.55e9 00 0 152. NA T2m 1018 K ## 5 1.55e9 00 0 69.1 NA T2m 1023 K ## 6 1.55e9 00 0 8.1 NA T2m 1025 K ## 7 1.55e9 00 0 5.2 NA T2m 1026 K ## 8 1.55e9 00 0 5.2 NA T2m 1027 K ## 9 1.55e9 00 0 28.5 NA T2m 1033 K ## 10 1.55e9 00 0 734. NA T2m 1035 K ## # … with 15,953 more rows, and 15 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, T2m &lt;dbl&gt; (precip_3h &lt;- join_to_fcst( precip_3h, read_point_obs( first_validdate(precip_3h), last_validdate(precip_3h), parameter = &quot;AccPcp3h&quot;, obs_path = here(&quot;data/OBSTABLE&quot;) ) )) ## ● MEPS_prod ## # A tibble: 6,369 x 22 ## fcdate fcst_cycle leadtime model_elevation parameter SID units validdate ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.55e9 00 3 -7 Pcp 1117 kg/m… 1.55e9 ## 2 1.55e9 00 3 -4.7 Pcp 1098 kg/m… 1.55e9 ## 3 1.55e9 00 3 -1.7 Pcp 6249 kg/m… 1.55e9 ## 4 1.55e9 00 3 -1.4 Pcp 6286 kg/m… 1.55e9 ## 5 1.55e9 00 3 -0.9 Pcp 2976 kg/m… 1.55e9 ## 6 1.55e9 00 3 -0.8 Pcp 2382 kg/m… 1.55e9 ## 7 1.55e9 00 3 -0.8 Pcp 2575 kg/m… 1.55e9 ## 8 1.55e9 00 3 -0.8 Pcp 2667 kg/m… 1.55e9 ## 9 1.55e9 00 3 -0.8 Pcp 2964 kg/m… 1.55e9 ## 10 1.55e9 00 3 -0.7 Pcp 2498 kg/m… 1.55e9 ## # … with 6,359 more rows, and 14 more variables: MEPS_prod_mbr000 &lt;dbl&gt;, ## # MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, MEPS_prod_mbr003 &lt;dbl&gt;, ## # MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, MEPS_prod_mbr006 &lt;dbl&gt;, ## # MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, MEPS_prod_mbr009 &lt;dbl&gt;, ## # lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, AccPcp3h &lt;dbl&gt; We can now add the observations to our plots plot_station_eps(t2m, 1492, 2019021700, obs_column = T2m, colour = &quot;red&quot;, shape = 21) Note this function still needs more work, and doesn’t work well in all cases! We can also make scatter plots of how forecasts compare with observations. They are made with hexbin plots and it is posisible to compare the whole ensemble with the observations or each ensemble member. The function is plot_scatter . plot_scatter(t2m, &quot;MEPS_prod&quot;, T2m) Your turn: Can you work out how to combine all members inta a single scatter plot. Check the help and note that the explanations are not complete! Make a scatter plots for each member for observed 3h precipitation &gt; 0.25 mm. Solutions: Can you work out how to combine all members inta a single scatter plot. Check the help and note that the explanations are not complete! plot_scatter(t2m, &quot;MEPS_prod&quot;, T2m, facet_members = FALSE) Make a scatter plots for each member for observed 3h precipitation &gt; 0.25 mm. plot_scatter(filter(precip_3h, AccPcp3h &gt; 0.25), &quot;MEPS_prod&quot;, AccPcp3h) 4.3.1 Multi model ensembles When we generated SQLite FCTABLE files for a fake multimodel ensemble we needed to specify a lot of details about the ensemble, such as the names and member numbers for each sub model. When we read those data in from the FCTABLE files, that information is already there. So, let’s read in our multi model ensemble and see what we get. read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;awesome_multimodel_eps&quot;, fcst_type = &quot;EPS&quot;, parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), by = &quot;6h&quot;, file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot; ) ## ● awesome_multimodel_eps ## ● AROME_Arctic_prod ## # A tibble: 4,580 x 6 ## SID fcdate leadtime validdate fcst_cycle AROME_Arctic_prod_mbr000 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 1550361600 0 1550361600 00 270. ## 2 1010 1550361600 0 1550361600 00 275. ## 3 1014 1550361600 0 1550361600 00 273. ## 4 1015 1550361600 0 1550361600 00 274. ## 5 1018 1550361600 0 1550361600 00 271. ## 6 1023 1550361600 0 1550361600 00 272. ## 7 1025 1550361600 0 1550361600 00 272. ## 8 1026 1550361600 0 1550361600 00 272. ## 9 1027 1550361600 0 1550361600 00 273. ## 10 1033 1550361600 0 1550361600 00 273. ## # … with 4,570 more rows ## ## ● MEPS_prod ## # A tibble: 4,580 x 8 ## SID fcdate leadtime validdate fcst_cycle MEPS_prod_mbr000 MEPS_prod_mbr001 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 00 268. 269. ## 2 1010 1.55e9 0 1.55e9 00 275. 276. ## 3 1014 1.55e9 0 1.55e9 00 273. 273. ## 4 1015 1.55e9 0 1.55e9 00 275. 276. ## 5 1018 1.55e9 0 1.55e9 00 271. 272. ## 6 1023 1.55e9 0 1.55e9 00 272. 272. ## 7 1025 1.55e9 0 1.55e9 00 272. 273. ## 8 1026 1.55e9 0 1.55e9 00 272. 272. ## 9 1027 1.55e9 0 1.55e9 00 273. 273. ## 10 1033 1.55e9 0 1.55e9 00 274. 274. ## # … with 4,570 more rows, and 1 more variable: MEPS_prod_mbr002 &lt;dbl&gt; As you can see we get the two sub models - AROME_Arctic_prod with one member, and MEPS_prod with 3 members - but not the full ensemble. However, we can create the full ensemble with the function merge_multimodel. (t2m_mm &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;awesome_multimodel_eps&quot;, fcst_type = &quot;EPS&quot;, parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), by = &quot;6h&quot;, file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot; ) %&gt;% merge_multimodel()) ## ● awesome_multimodel_eps ## # A tibble: 4,580 x 9 ## SID fcdate leadtime validdate fcst_cycle AROME_Arctic_pr… MEPS_prod_mbr000 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 00 270. 268. ## 2 1010 1.55e9 0 1.55e9 00 275. 275. ## 3 1014 1.55e9 0 1.55e9 00 273. 273. ## 4 1015 1.55e9 0 1.55e9 00 274. 275. ## 5 1018 1.55e9 0 1.55e9 00 271. 271. ## 6 1023 1.55e9 0 1.55e9 00 272. 272. ## 7 1025 1.55e9 0 1.55e9 00 272. 272. ## 8 1026 1.55e9 0 1.55e9 00 272. 272. ## 9 1027 1.55e9 0 1.55e9 00 273. 273. ## 10 1033 1.55e9 0 1.55e9 00 273. 274. ## # … with 4,570 more rows, and 2 more variables: MEPS_prod_mbr001 &lt;dbl&gt;, ## # MEPS_prod_mbr002 &lt;dbl&gt; ## ## ● AROME_Arctic_prod(awesome_multimodel_eps) ## # A tibble: 4,580 x 6 ## SID fcdate leadtime validdate fcst_cycle AROME_Arctic_prod_mbr000 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 1550361600 0 1550361600 00 270. ## 2 1010 1550361600 0 1550361600 00 275. ## 3 1014 1550361600 0 1550361600 00 273. ## 4 1015 1550361600 0 1550361600 00 274. ## 5 1018 1550361600 0 1550361600 00 271. ## 6 1023 1550361600 0 1550361600 00 272. ## 7 1025 1550361600 0 1550361600 00 272. ## 8 1026 1550361600 0 1550361600 00 272. ## 9 1027 1550361600 0 1550361600 00 273. ## 10 1033 1550361600 0 1550361600 00 273. ## # … with 4,570 more rows ## ## ● MEPS_prod(awesome_multimodel_eps) ## # A tibble: 4,580 x 8 ## SID fcdate leadtime validdate fcst_cycle MEPS_prod_mbr000 MEPS_prod_mbr001 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 00 268. 269. ## 2 1010 1.55e9 0 1.55e9 00 275. 276. ## 3 1014 1.55e9 0 1.55e9 00 273. 273. ## 4 1015 1.55e9 0 1.55e9 00 275. 276. ## 5 1018 1.55e9 0 1.55e9 00 271. 272. ## 6 1023 1.55e9 0 1.55e9 00 272. 272. ## 7 1025 1.55e9 0 1.55e9 00 272. 273. ## 8 1026 1.55e9 0 1.55e9 00 272. 272. ## 9 1027 1.55e9 0 1.55e9 00 273. 273. ## 10 1033 1.55e9 0 1.55e9 00 274. 274. ## # … with 4,570 more rows, and 1 more variable: MEPS_prod_mbr002 &lt;dbl&gt; Now we have the full multimodel ensemble and the two sub models, all as separate models in the harp_fcst object. You can, if you prefer, discard the sub models by setting keep_sub_models = FALSE in merge_multimodel. Now we have two harp_fcst objects for 2m temperature, t2m and t2m_mm. We can easily combine them into a single harp_fcst using the standard concatenate function, c c(t2m, t2m_mm) ## ● MEPS_prod ## # A tibble: 15,963 x 23 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 2.6 NA T2m 1015 K ## 4 1.55e9 00 0 152. NA T2m 1018 K ## 5 1.55e9 00 0 69.1 NA T2m 1023 K ## 6 1.55e9 00 0 8.1 NA T2m 1025 K ## 7 1.55e9 00 0 5.2 NA T2m 1026 K ## 8 1.55e9 00 0 5.2 NA T2m 1027 K ## 9 1.55e9 00 0 28.5 NA T2m 1033 K ## 10 1.55e9 00 0 734. NA T2m 1035 K ## # … with 15,953 more rows, and 15 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, T2m &lt;dbl&gt; ## ## ● awesome_multimodel_eps ## # A tibble: 4,580 x 9 ## SID fcdate leadtime validdate fcst_cycle AROME_Arctic_pr… MEPS_prod_mbr000 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 00 270. 268. ## 2 1010 1.55e9 0 1.55e9 00 275. 275. ## 3 1014 1.55e9 0 1.55e9 00 273. 273. ## 4 1015 1.55e9 0 1.55e9 00 274. 275. ## 5 1018 1.55e9 0 1.55e9 00 271. 271. ## 6 1023 1.55e9 0 1.55e9 00 272. 272. ## 7 1025 1.55e9 0 1.55e9 00 272. 272. ## 8 1026 1.55e9 0 1.55e9 00 272. 272. ## 9 1027 1.55e9 0 1.55e9 00 273. 273. ## 10 1033 1.55e9 0 1.55e9 00 273. 274. ## # … with 4,570 more rows, and 2 more variables: MEPS_prod_mbr001 &lt;dbl&gt;, ## # MEPS_prod_mbr002 &lt;dbl&gt; ## ## ● AROME_Arctic_prod(awesome_multimodel_eps) ## # A tibble: 4,580 x 6 ## SID fcdate leadtime validdate fcst_cycle AROME_Arctic_prod_mbr000 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 1550361600 0 1550361600 00 270. ## 2 1010 1550361600 0 1550361600 00 275. ## 3 1014 1550361600 0 1550361600 00 273. ## 4 1015 1550361600 0 1550361600 00 274. ## 5 1018 1550361600 0 1550361600 00 271. ## 6 1023 1550361600 0 1550361600 00 272. ## 7 1025 1550361600 0 1550361600 00 272. ## 8 1026 1550361600 0 1550361600 00 272. ## 9 1027 1550361600 0 1550361600 00 273. ## 10 1033 1550361600 0 1550361600 00 273. ## # … with 4,570 more rows ## ## ● MEPS_prod(awesome_multimodel_eps) ## # A tibble: 4,580 x 8 ## SID fcdate leadtime validdate fcst_cycle MEPS_prod_mbr000 MEPS_prod_mbr001 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 00 268. 269. ## 2 1010 1.55e9 0 1.55e9 00 275. 276. ## 3 1014 1.55e9 0 1.55e9 00 273. 273. ## 4 1015 1.55e9 0 1.55e9 00 275. 276. ## 5 1018 1.55e9 0 1.55e9 00 271. 272. ## 6 1023 1.55e9 0 1.55e9 00 272. 272. ## 7 1025 1.55e9 0 1.55e9 00 272. 273. ## 8 1026 1.55e9 0 1.55e9 00 272. 272. ## 9 1027 1.55e9 0 1.55e9 00 273. 273. ## 10 1033 1.55e9 0 1.55e9 00 274. 274. ## # … with 4,570 more rows, and 1 more variable: MEPS_prod_mbr002 &lt;dbl&gt; 4.3.2 Lagged ensembles Reading lagged ensembles from FCTABLE files is also easier than in the conversion to SQLite step. Since FCTABLE files were saved for each forecast cycle, and the members for that cycle in the file, all we need to do is tell read_point_forecast the appropriate lags. Here we need to be a bit careful about what we’re doing, especially if we are reading in data from other models at the same. In the case of CMEPS there is a new set of ensemble members every three hours, but if we want to compare it with MEPS, which has a full set of members every six hours, which should set the by = \"6h\" and use our lags specification to ensure we read in the correct amount of data for CMEPS, by ensuring there are lags from zero to five hours. This probably becomes clearer with an example… (t2m &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = c(&quot;CMEPS_prod&quot;, &quot;MEPS_prod&quot;), fcst_type = &quot;EPS&quot;, parameter = &quot;T2m&quot;, lead_time = seq(0, 12, 3), by = &quot;6h&quot;, lags = list( CMEPS_prod = paste0(seq(0, 5), &quot;h&quot;), MEPS_prod = &quot;0h&quot; ), file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot;, merge_lags = FALSE )) ## ● CMEPS_prod ## # A tibble: 110,304 x 15 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 29.1 NA T2m 1014 K ## 4 1.55e9 00 0 2.6 NA T2m 1015 K ## 5 1.55e9 00 0 152. NA T2m 1018 K ## 6 1.55e9 00 0 69.1 NA T2m 1023 K ## 7 1.55e9 00 0 8.1 NA T2m 1025 K ## 8 1.55e9 00 0 5.2 NA T2m 1026 K ## 9 1.55e9 00 0 5.2 NA T2m 1027 K ## 10 1.55e9 00 0 28.5 NA T2m 1033 K ## # … with 110,294 more rows, and 7 more variables: validdate &lt;int&gt;, ## # CMEPS_prod_mbr000 &lt;dbl&gt;, CMEPS_prod_mbr001 &lt;dbl&gt;, CMEPS_prod_mbr003 &lt;dbl&gt;, ## # CMEPS_prod_mbr004 &lt;dbl&gt;, CMEPS_prod_mbr005 &lt;dbl&gt;, CMEPS_prod_mbr006 &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 22,980 x 19 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 29.1 NA T2m 1014 K ## 4 1.55e9 00 0 2.6 NA T2m 1015 K ## 5 1.55e9 00 0 152. NA T2m 1018 K ## 6 1.55e9 00 0 69.1 NA T2m 1023 K ## 7 1.55e9 00 0 8.1 NA T2m 1025 K ## 8 1.55e9 00 0 5.2 NA T2m 1026 K ## 9 1.55e9 00 0 5.2 NA T2m 1027 K ## 10 1.55e9 00 0 28.5 NA T2m 1033 K ## # … with 22,970 more rows, and 11 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt; There was a warning about missing files. This is because we didn’t generate any FCTABLE files for the 21 cycle on the 16 Feb 2019 and they were also not uploaded to this project! However, this illustrates that when lagging is used, you have to be very clear in your mind exactly what you need to get the lagged ensemble you want. You will also see that there a are lot of missing data - this is immediately apparent for members 5 and 6. This is becasuse, since we set merge_lags = FALSE, we have only read in lagged data, but haven’t created the lagged ensemble yet. We create the lagged ensemble with the function lag_forecast. We tell the function which cycles you want to be the “parent” cycles and the function will shift all of the members between the parent cycles to the parent cycle. By default, children are found by looking backwards in time, as would happen in an operational setting - for example, if the parent cycles are 0, 6, 12 and 18, the child cycles for the parent at 6 are at 5, 4, 3, 2, and 1, and so on for the other parents. You can look for children in the other direction by setting direction = -1. Note that when merge_lags = TRUE, the ensemble is created with the parent cycles set to be those generated from the start_date, end_date and by arguments. lag_forecast( t2m, fcst_model = &quot;CMEPS_prod&quot;, parent_cycles = seq(0, 18, 3) ) ## ● CMEPS_prod ## # A tibble: 36,768 x 14 ## fcst_cycle fcdate leadtime model_elevation parameter SID units validdate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 00 1.55e9 0 36.4 T2m 1001 K 1.55e9 ## 2 00 1.55e9 0 4.1 T2m 1010 K 1.55e9 ## 3 00 1.55e9 0 29.1 T2m 1014 K 1.55e9 ## 4 00 1.55e9 0 2.6 T2m 1015 K 1.55e9 ## 5 00 1.55e9 0 152. T2m 1018 K 1.55e9 ## 6 00 1.55e9 0 69.1 T2m 1023 K 1.55e9 ## 7 00 1.55e9 0 8.1 T2m 1025 K 1.55e9 ## 8 00 1.55e9 0 5.2 T2m 1026 K 1.55e9 ## 9 00 1.55e9 0 5.2 T2m 1027 K 1.55e9 ## 10 00 1.55e9 0 28.5 T2m 1033 K 1.55e9 ## # … with 36,758 more rows, and 6 more variables: CMEPS_prod_mbr000 &lt;dbl&gt;, ## # CMEPS_prod_mbr001 &lt;dbl&gt;, CMEPS_prod_mbr003 &lt;dbl&gt;, CMEPS_prod_mbr004 &lt;dbl&gt;, ## # CMEPS_prod_mbr005 &lt;dbl&gt;, CMEPS_prod_mbr006 &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 22,980 x 19 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA T2m 1001 K ## 2 1.55e9 00 0 4.1 NA T2m 1010 K ## 3 1.55e9 00 0 29.1 NA T2m 1014 K ## 4 1.55e9 00 0 2.6 NA T2m 1015 K ## 5 1.55e9 00 0 152. NA T2m 1018 K ## 6 1.55e9 00 0 69.1 NA T2m 1023 K ## 7 1.55e9 00 0 8.1 NA T2m 1025 K ## 8 1.55e9 00 0 5.2 NA T2m 1026 K ## 9 1.55e9 00 0 5.2 NA T2m 1027 K ## 10 1.55e9 00 0 28.5 NA T2m 1033 K ## # … with 22,970 more rows, and 11 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt; Your turn: Use pull(.fcst, fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) to see which cycles you are left with when you try different parent_cycles and direction = 1 or direction = -1. Try to remember what we actually did when we ran read_eps_interpolate for CMEPS Combine 2m temperature forecasts for the multimodel ensemble (including sub models), MEPS_prod and CMEPS(with parent_cycles of 6, 12 and 18) and select the common cases and join the observations (note you will have to run set_units(t2m_mm, \"K\") due to a bug in read_point_forecast that drops that units column for multi model ensembles) Solutions Use pull(.fcst, fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) to see which cycles you are left with when you try different parent_cycles and direction = 1 or direction = -1. pull(t2m, fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) ## $CMEPS_prod ## [1] &quot;00&quot; &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;04&quot; &quot;05&quot; &quot;06&quot; &quot;07&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ## [16] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;22&quot; &quot;23&quot; ## ## $MEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; pull(lag_forecast(t2m, &quot;CMEPS_prod&quot;, seq(0, 18, 6)), fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) ## $CMEPS_prod ## [1] &quot;06&quot; &quot;12&quot; &quot;18&quot; ## ## $MEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; pull(lag_forecast(t2m, &quot;CMEPS_prod&quot;, seq(0, 18, 3)), fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) ## $CMEPS_prod ## [1] &quot;00&quot; &quot;03&quot; &quot;06&quot; &quot;09&quot; &quot;12&quot; &quot;15&quot; &quot;18&quot; ## ## $MEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; pull(lag_forecast(t2m, &quot;CMEPS_prod&quot;, seq(0, 18, 6), direction = -1), fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) ## $CMEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; ## ## $MEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; pull(lag_forecast(t2m, &quot;CMEPS_prod&quot;, seq(0, 18, 3), direction = -1), fcst_cycle) %&gt;% map(unique) %&gt;% map(sort) ## $CMEPS_prod ## [1] &quot;00&quot; &quot;03&quot; &quot;06&quot; &quot;09&quot; &quot;12&quot; &quot;15&quot; ## ## $MEPS_prod ## [1] &quot;00&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; Combine 2m temperature forecasts for the multimodel ensemble (including sub models), MEPS_prod and CMEPS(with parent_cycles of 0, 6, 12 and 18) and select the common cases (t2m &lt;- c(set_units(t2m_mm, &quot;K&quot;), lag_forecast(t2m, &quot;CMEPS_prod&quot;, seq(0, 18, 6))) %&gt;% common_cases() %&gt;% join_to_fcst( read_point_obs( first_validdate(.), last_validdate(.), parameter = &quot;T2m&quot;, obs_path = here(&quot;data/OBSTABLE&quot;) ) ) ) ## ● awesome_multimodel_eps ## # A tibble: 1,955 x 14 ## SID fcdate leadtime validdate fcst_cycle units AROME_Arctic_pr… ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 06 K 271. ## 2 1010 1.55e9 0 1.55e9 06 K 273. ## 3 1015 1.55e9 0 1.55e9 06 K 274. ## 4 1018 1.55e9 0 1.55e9 06 K 268. ## 5 1023 1.55e9 0 1.55e9 06 K 269. ## 6 1025 1.55e9 0 1.55e9 06 K 270. ## 7 1026 1.55e9 0 1.55e9 06 K 269. ## 8 1027 1.55e9 0 1.55e9 06 K 270. ## 9 1033 1.55e9 0 1.55e9 06 K 272. ## 10 1035 1.55e9 0 1.55e9 06 K 265. ## # … with 1,945 more rows, and 7 more variables: MEPS_prod_mbr000 &lt;dbl&gt;, ## # MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, ## # elev &lt;dbl&gt;, T2m &lt;dbl&gt; ## ## ● AROME_Arctic_prod(awesome_multimodel_eps) ## # A tibble: 1,955 x 11 ## SID fcdate leadtime validdate fcst_cycle units AROME_Arctic_pr… lon lat ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 06 K 271. -8.67 70.9 ## 2 1010 1.55e9 0 1.55e9 06 K 273. 16.1 69.3 ## 3 1015 1.55e9 0 1.55e9 06 K 274. 17.8 69.6 ## 4 1018 1.55e9 0 1.55e9 06 K 268. 16.0 69.2 ## 5 1023 1.55e9 0 1.55e9 06 K 269. 18.5 69.1 ## 6 1025 1.55e9 0 1.55e9 06 K 270. 18.9 69.7 ## 7 1026 1.55e9 0 1.55e9 06 K 269. 18.9 69.7 ## 8 1027 1.55e9 0 1.55e9 06 K 270. 18.9 69.7 ## 9 1033 1.55e9 0 1.55e9 06 K 272. 19.5 70.2 ## 10 1035 1.55e9 0 1.55e9 06 K 265. 20.1 69.6 ## # … with 1,945 more rows, and 2 more variables: elev &lt;dbl&gt;, T2m &lt;dbl&gt; ## ## ● MEPS_prod(awesome_multimodel_eps) ## # A tibble: 1,955 x 13 ## SID fcdate leadtime validdate fcst_cycle units MEPS_prod_mbr000 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1001 1.55e9 0 1.55e9 06 K 270. ## 2 1010 1.55e9 0 1.55e9 06 K 273. ## 3 1015 1.55e9 0 1.55e9 06 K 273. ## 4 1018 1.55e9 0 1.55e9 06 K 269. ## 5 1023 1.55e9 0 1.55e9 06 K 269. ## 6 1025 1.55e9 0 1.55e9 06 K 269. ## 7 1026 1.55e9 0 1.55e9 06 K 268. ## 8 1027 1.55e9 0 1.55e9 06 K 269. ## 9 1033 1.55e9 0 1.55e9 06 K 271. ## 10 1035 1.55e9 0 1.55e9 06 K 265. ## # … with 1,945 more rows, and 6 more variables: MEPS_prod_mbr001 &lt;dbl&gt;, ## # MEPS_prod_mbr002 &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, T2m &lt;dbl&gt; ## ## ● CMEPS_prod ## # A tibble: 1,955 x 24 ## fcst_cycle fcdate leadtime model_elevation parameter SID units validdate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 06 1.55e9 0 36.4 T2m 1001 K 1.55e9 ## 2 06 1.55e9 0 4.1 T2m 1010 K 1.55e9 ## 3 06 1.55e9 0 2.6 T2m 1015 K 1.55e9 ## 4 06 1.55e9 0 152. T2m 1018 K 1.55e9 ## 5 06 1.55e9 0 69.1 T2m 1023 K 1.55e9 ## 6 06 1.55e9 0 8.1 T2m 1025 K 1.55e9 ## 7 06 1.55e9 0 5.2 T2m 1026 K 1.55e9 ## 8 06 1.55e9 0 5.2 T2m 1027 K 1.55e9 ## 9 06 1.55e9 0 28.5 T2m 1033 K 1.55e9 ## 10 06 1.55e9 0 734. T2m 1035 K 1.55e9 ## # … with 1,945 more rows, and 16 more variables: CMEPS_prod_mbr000 &lt;dbl&gt;, ## # CMEPS_prod_mbr001 &lt;dbl&gt;, CMEPS_prod_mbr003 &lt;dbl&gt;, CMEPS_prod_mbr004 &lt;dbl&gt;, ## # CMEPS_prod_mbr005 &lt;dbl&gt;, CMEPS_prod_mbr006 &lt;dbl&gt;, ## # CMEPS_prod_mbr000_lag &lt;dbl&gt;, CMEPS_prod_mbr001_lag &lt;dbl&gt;, ## # CMEPS_prod_mbr003_lag &lt;dbl&gt;, CMEPS_prod_mbr004_lag &lt;dbl&gt;, ## # CMEPS_prod_mbr005_lag &lt;dbl&gt;, CMEPS_prod_mbr006_lag &lt;dbl&gt;, lon &lt;dbl&gt;, ## # lat &lt;dbl&gt;, elev &lt;dbl&gt;, T2m &lt;dbl&gt; ## ## ● MEPS_prod ## # A tibble: 1,955 x 23 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 06 0 36.4 NA T2m 1001 K ## 2 1.55e9 06 0 4.1 NA T2m 1010 K ## 3 1.55e9 06 0 2.6 NA T2m 1015 K ## 4 1.55e9 06 0 152. NA T2m 1018 K ## 5 1.55e9 06 0 69.1 NA T2m 1023 K ## 6 1.55e9 06 0 8.1 NA T2m 1025 K ## 7 1.55e9 06 0 5.2 NA T2m 1026 K ## 8 1.55e9 06 0 5.2 NA T2m 1027 K ## 9 1.55e9 06 0 28.5 NA T2m 1033 K ## 10 1.55e9 06 0 734. NA T2m 1035 K ## # … with 1,945 more rows, and 15 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, elev &lt;dbl&gt;, T2m &lt;dbl&gt; Now we can try plotting the ensmeble temperature evolution for REIPA again plot_station_eps( t2m, 1114, 2019021706, obs_column = T2m, colour = &quot;red&quot;, shape = 21 ) + scale_x_continuous(breaks = seq(0, 9, 3)) There are still a few problems with plot_station_eps so tread carefully! 4.3.3 Shifted forecasts Let’s say you want to compare an ensmeble with another but with the time shifted. i.e. how does a forecast that is 6 hours old compare with the current forecast. We can do that by shifting the forecast - that means adjusting the forecast start time and the forecast lead time by 6 hours so the forecast appears to have been run at a different time. We can do this with shift_forecast. To illustrate, we read in the MEPS_prod data, for 10m wind speed, just to be different! s10m &lt;- read_point_forecast( start_date = 2019021700, end_date = 2019021718, fcst_model = &quot;MEPS_prod&quot;, fcst_type = &quot;EPS&quot;, parameter = &quot;S10m&quot;, lead_time = seq(0, 12, 3), file_path = here(&quot;data/FCTABLE/ensemble&quot;), file_template = &quot;fctable_eps_all_leads&quot; ) And then we are going to create a new ensemble with MEPS_prod shifted by 6 hours, so the forecast that started at 00 UTC now appears to have started at 06 UTC etc. This means that the forecast with a lead time of 6 hours, that used to start at 00 UTC now appears to have started at 06 UTC and has a lead time of 0 hours. (s10m &lt;- shift_forecast( s10m, fcst_shifts = list(MEPS_prod = 6), keep_unshifted = TRUE )) ## ● MEPS_prod ## # A tibble: 24,060 x 19 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 00 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 00 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 00 0 29.1 NA S10m 1014 m/s ## 4 1.55e9 00 0 2.6 NA S10m 1015 m/s ## 5 1.55e9 00 0 152. NA S10m 1018 m/s ## 6 1.55e9 00 0 16.4 NA S10m 1021 m/s ## 7 1.55e9 00 0 39.5 NA S10m 1022 m/s ## 8 1.55e9 00 0 69.1 NA S10m 1023 m/s ## 9 1.55e9 00 0 8.1 NA S10m 1025 m/s ## 10 1.55e9 00 0 5.2 NA S10m 1026 m/s ## # … with 24,050 more rows, and 11 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt; ## ## ● MEPS_prod_shifted_6h ## # A tibble: 14,436 x 19 ## fcdate fcst_cycle leadtime model_elevation p parameter SID units ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1.55e9 06 0 36.4 NA S10m 1001 m/s ## 2 1.55e9 06 0 4.1 NA S10m 1010 m/s ## 3 1.55e9 06 0 29.1 NA S10m 1014 m/s ## 4 1.55e9 06 0 2.6 NA S10m 1015 m/s ## 5 1.55e9 06 0 152. NA S10m 1018 m/s ## 6 1.55e9 06 0 16.4 NA S10m 1021 m/s ## 7 1.55e9 06 0 39.5 NA S10m 1022 m/s ## 8 1.55e9 06 0 69.1 NA S10m 1023 m/s ## 9 1.55e9 06 0 8.1 NA S10m 1025 m/s ## 10 1.55e9 06 0 5.2 NA S10m 1026 m/s ## # … with 14,426 more rows, and 11 more variables: validdate &lt;int&gt;, ## # MEPS_prod_mbr000 &lt;dbl&gt;, MEPS_prod_mbr001 &lt;dbl&gt;, MEPS_prod_mbr002 &lt;dbl&gt;, ## # MEPS_prod_mbr003 &lt;dbl&gt;, MEPS_prod_mbr004 &lt;dbl&gt;, MEPS_prod_mbr005 &lt;dbl&gt;, ## # MEPS_prod_mbr006 &lt;dbl&gt;, MEPS_prod_mbr007 &lt;dbl&gt;, MEPS_prod_mbr008 &lt;dbl&gt;, ## # MEPS_prod_mbr009 &lt;dbl&gt; So now we have two ensembles in our data: MEPS_prod and MEPS_prod_shifted_6h. We can see that they are different by getting the start times (fcdate) and lead times from each of the forecasts. transmute(s10m, fcdate = unix2datetime(fcdate), leadtime) %&gt;% map(distinct) ## $MEPS_prod ## # A tibble: 20 x 2 ## fcdate leadtime ## &lt;dttm&gt; &lt;int&gt; ## 1 2019-02-17 00:00:00 0 ## 2 2019-02-17 00:00:00 3 ## 3 2019-02-17 00:00:00 6 ## 4 2019-02-17 00:00:00 9 ## 5 2019-02-17 00:00:00 12 ## 6 2019-02-17 06:00:00 0 ## 7 2019-02-17 06:00:00 3 ## 8 2019-02-17 06:00:00 6 ## 9 2019-02-17 06:00:00 9 ## 10 2019-02-17 06:00:00 12 ## 11 2019-02-17 12:00:00 0 ## 12 2019-02-17 12:00:00 3 ## 13 2019-02-17 12:00:00 6 ## 14 2019-02-17 12:00:00 9 ## 15 2019-02-17 12:00:00 12 ## 16 2019-02-17 18:00:00 0 ## 17 2019-02-17 18:00:00 3 ## 18 2019-02-17 18:00:00 6 ## 19 2019-02-17 18:00:00 9 ## 20 2019-02-17 18:00:00 12 ## ## $MEPS_prod_shifted_6h ## # A tibble: 12 x 2 ## fcdate leadtime ## &lt;dttm&gt; &lt;dbl&gt; ## 1 2019-02-17 06:00:00 0 ## 2 2019-02-17 06:00:00 3 ## 3 2019-02-17 06:00:00 6 ## 4 2019-02-17 12:00:00 0 ## 5 2019-02-17 12:00:00 3 ## 6 2019-02-17 12:00:00 6 ## 7 2019-02-17 18:00:00 0 ## 8 2019-02-17 18:00:00 3 ## 9 2019-02-17 18:00:00 6 ## 10 2019-02-18 00:00:00 0 ## 11 2019-02-18 00:00:00 3 ## 12 2019-02-18 00:00:00 6 We’re going to want to verify s10m later, so let’s make sure we have the common cases and get the observations s10m &lt;- common_cases(s10m) %&gt;% join_to_fcst( read_point_obs( first_validdate(s10m), last_validdate(s10m), parameter = &quot;S10m&quot;, obs_path = here(&quot;data/OBSTABLE&quot;) ) ) 4.3.4 Verification For ensemble verification, the main verification function is ens_verify. Similar to det_verify, you give it the data and the column name of the observations. ens_verify(t2m, T2m) ## $ens_summary_scores ## # A tibble: 16 x 12 ## mname leadtime num_cases mean_bias stde rmse spread spread_skill_ra… ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 awes… 0 467 0.177 1.40 1.41 0.744 0.526 ## 2 awes… 3 509 0.552 2.09 2.16 0.713 0.330 ## 3 awes… 6 472 0.596 2.34 2.42 0.710 0.294 ## 4 awes… 9 507 0.782 2.33 2.46 0.798 0.325 ## 5 MEPS… 0 467 0.187 1.55 1.56 0.756 0.486 ## 6 MEPS… 3 509 0.572 2.09 2.16 0.754 0.348 ## 7 MEPS… 6 472 0.612 2.35 2.42 0.734 0.303 ## 8 MEPS… 9 507 0.798 2.32 2.45 0.811 0.330 ## 9 CMEP… 0 467 0.0678 1.67 1.67 0.686 0.410 ## 10 CMEP… 3 509 0.344 2.11 2.14 0.729 0.341 ## 11 CMEP… 6 472 0.470 2.33 2.37 0.766 0.323 ## 12 CMEP… 9 507 0.673 2.34 2.44 0.806 0.331 ## 13 MEPS… 0 467 0.139 1.55 1.56 0.618 0.397 ## 14 MEPS… 3 509 0.457 2.06 2.11 0.722 0.342 ## 15 MEPS… 6 472 0.498 2.30 2.35 0.770 0.327 ## 16 MEPS… 9 507 0.670 2.30 2.39 0.882 0.368 ## # … with 4 more variables: rank_histogram &lt;list&gt;, crps &lt;dbl&gt;, ## # crps_potential &lt;dbl&gt;, crps_reliability &lt;dbl&gt; ## ## $ens_threshold_scores ## data frame with 0 columns and 0 rows ## ## $det_summary_scores ## # A tibble: 120 x 9 ## mname leadtime member sub_model num_cases bias rmse mae stde ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 awesome_mul… 0 mbr000 AROME_Arcti… 467 0.145 1.12 0.738 1.11 ## 2 awesome_mul… 3 mbr000 AROME_Arcti… 509 0.493 2.24 1.48 2.19 ## 3 awesome_mul… 6 mbr000 AROME_Arcti… 472 0.550 2.49 1.65 2.43 ## 4 awesome_mul… 9 mbr000 AROME_Arcti… 507 0.733 2.58 1.72 2.47 ## 5 awesome_mul… 0 mbr000 MEPS_prod 467 0.155 1.55 1.09 1.55 ## 6 awesome_mul… 3 mbr000 MEPS_prod 509 0.479 2.11 1.42 2.05 ## 7 awesome_mul… 6 mbr000 MEPS_prod 472 0.525 2.32 1.55 2.26 ## 8 awesome_mul… 9 mbr000 MEPS_prod 507 0.689 2.39 1.63 2.30 ## 9 awesome_mul… 0 mbr001 MEPS_prod 467 -0.0784 1.59 1.17 1.59 ## 10 awesome_mul… 3 mbr001 MEPS_prod 509 0.399 2.03 1.39 1.99 ## # … with 110 more rows ## ## attr(,&quot;parameter&quot;) ## [1] &quot;T2m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;2019021706&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;2019021718&quot; ## attr(,&quot;num_stations&quot;) ## [1] 180 By default, summary deterministic verification scores will be computed for each member. We can also compute categorical ensemble scores based on thresholds (such as Brier Score, ROC, economic value etc.) if we pass some thresholds to the function verif_s10m &lt;- ens_verify(s10m, S10m, thresholds = c(1, 2, 5, 8)) You can also compute “fair” scores for CRPS and Brier Score, which take account differences in the number of members between ensembles by scaling them to an equal number of members. We do this by passing num_ref_members. verif_t2m &lt;- ens_verify( t2m, T2m, thresholds = seq(262, 272, 2), num_ref_members = 10 ) So now we can have a quick look at some score plots using plot_point_verif - the default for the type of verification is verif_type = \"ens\" so we don’t need to specify it (for now) plot_point_verif(verif_s10m, crps) plot_point_verif(verif_s10m, spread_skill) plot_point_verif(verif_s10m, rank_histogram) plot_point_verif(verif_t2m, spread_skill_ratio) plot_point_verif(verif_t2m, fair_crps) plot_point_verif( verif_t2m, rank_histogram, facet_by = vars(leadtime), num_facet_cols = 2, num_legend_rows = 2 ) + theme(axis.text.x = element_text(angle = 90)) plot_point_verif(verif_t2m, brier_score, facet_by = vars(threshold)) We can also set the colours / fill colours for our plots by passing a data frame with one column as mname and the other as colour. The easiest way to get the model names for the mname is either from the forecast object using names, or if that is not available at the time you can get if from the verification list. names(t2m) ## [1] &quot;awesome_multimodel_eps&quot; ## [2] &quot;AROME_Arctic_prod(awesome_multimodel_eps)&quot; ## [3] &quot;MEPS_prod(awesome_multimodel_eps)&quot; ## [4] &quot;CMEPS_prod&quot; ## [5] &quot;MEPS_prod&quot; map(verif_t2m, pull, mname) %&gt;% reduce(union) ## [1] &quot;awesome_multimodel_eps&quot; ## [2] &quot;MEPS_prod(awesome_multimodel_eps)&quot; ## [3] &quot;CMEPS_prod&quot; ## [4] &quot;MEPS_prod&quot; ## [5] &quot;AROME_Arctic_prod(awesome_multimodel_eps)&quot; my_colours &lt;- tibble( mname = names(t2m), colour = c(&quot;darkblue&quot;, &quot;blue&quot;, &quot;skyblue&quot;, &quot;red&quot;, &quot;green&quot;) ) We can also plot the deterministic scores for each member. Here we need to tell plot_point_verif to do faceting by mname and to colour the lines according to the ensemble member. plot_point_verif( verif_t2m, mae, verif_type = &quot;det&quot;, facet_by = vars(mname), colour_by = member, legend_position = &quot;right&quot;, num_legend_rows = 20 ) We’ve got quite a lot going on there and it’s not a useful for plot, put we can clean it up by adding some more grouping variables. First let’s differentiate between the control members and the perturbed members. verif_t2m$det_summary_scores &lt;- mutate( verif_t2m$det_summary_scores, member_type = case_when( grepl(&quot;000&quot;, member) ~ &quot;control&quot;, TRUE ~ &quot;perturbed&quot; ) ) member_colours &lt;- tribble( ~member_type, ~colour, &quot;control&quot;, &quot;red&quot;, &quot;perturbed&quot;, &quot;grey70&quot; ) plot_point_verif( verif_t2m, mae, verif_type = &quot;det&quot;, facet_by = vars(mname), colour_by = member_type, colour_table = member_colours, legend_position = &quot;right&quot;, num_legend_rows = 2, group = member ) And then we’re going to add an extra column to the data frame so that we can give lagged members a different line type verif_t2m$det_summary_scores &lt;- mutate( verif_t2m$det_summary_scores, lagging = case_when( grepl(&quot;_lag$&quot;, member) ~ &quot;lagged&quot;, TRUE ~ &quot;unlagged&quot; ) ) plot_point_verif( verif_t2m , mae, verif_type = &quot;det&quot;, facet_by = vars(mname), colour_by = member_type, colour_table = member_colours, linetype_by = fct_rev(lagging), legend_position = &quot;right&quot;, num_legend_rows = 2, group = member ) And finally, although our multi model ensemble plot looks strange, because it has 2 member 0s, that’s OK because we don’t need it as the data are in the plots for the sub models. plot_point_verif( verif_t2m , mae, verif_type = &quot;det&quot;, facet_by = vars(mname), colour_by = member_type, colour_table = member_colours, linetype_by = fct_rev(lagging), legend_position = &quot;right&quot;, num_legend_rows = 2, group = member, filter_by = vars(mname != &quot;awesome_multimodel_eps&quot;), num_facet_cols = 2 ) + scale_x_continuous(breaks = seq(0, 9, 3)) 4.3.5 Verification by groups As we mentioned in the deterministic section, we can compute verification scores for groups of data. So in our data we have forecasts from 3 different cycles: 06, 12, and 18 UTC. (Let’s forget for a moment that also means only 3 forecasts and assume that the data include forecasts from several days running with the same start times every day). We can use the groupings argument to say that we want to verify for each lead time for each forecast cycle. ens_verify(t2m, T2m, groupings = c(&quot;leadtime&quot;, &quot;fcst_cycle&quot;)) ## $ens_summary_scores ## # A tibble: 48 x 13 ## mname leadtime fcst_cycle num_cases mean_bias stde rmse spread ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 awes… 0 06 147 0.0532 1.02 1.01 0.525 ## 2 awes… 0 12 166 -0.131 1.01 1.01 0.523 ## 3 awes… 0 18 154 0.626 1.90 1.99 1.06 ## 4 awes… 3 06 170 -0.0961 1.33 1.33 0.505 ## 5 awes… 3 12 169 0.825 1.76 1.93 0.542 ## 6 awes… 3 18 170 0.929 2.77 2.91 0.988 ## 7 awes… 6 06 166 -0.337 1.22 1.26 0.499 ## 8 awes… 6 12 154 1.23 2.48 2.76 0.627 ## 9 awes… 6 18 152 0.970 2.79 2.95 0.946 ## 10 awes… 9 06 169 0.708 1.81 1.94 0.624 ## # … with 38 more rows, and 5 more variables: spread_skill_ratio &lt;dbl&gt;, ## # rank_histogram &lt;list&gt;, crps &lt;dbl&gt;, crps_potential &lt;dbl&gt;, ## # crps_reliability &lt;dbl&gt; ## ## $ens_threshold_scores ## data frame with 0 columns and 0 rows ## ## $det_summary_scores ## # A tibble: 360 x 10 ## mname leadtime fcst_cycle member sub_model num_cases bias rmse mae ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 awes… 0 06 mbr000 AROME_Ar… 147 0.0882 0.836 0.526 ## 2 awes… 3 06 mbr000 AROME_Ar… 170 -0.139 1.36 1.03 ## 3 awes… 6 06 mbr000 AROME_Ar… 166 -0.307 1.30 0.981 ## 4 awes… 9 06 mbr000 AROME_Ar… 169 0.708 1.91 1.34 ## 5 awes… 0 12 mbr000 AROME_Ar… 166 0.0626 0.825 0.560 ## 6 awes… 3 12 mbr000 AROME_Ar… 169 0.788 1.97 1.37 ## 7 awes… 6 12 mbr000 AROME_Ar… 154 1.25 2.84 2.06 ## 8 awes… 9 12 mbr000 AROME_Ar… 170 0.907 3.26 2.25 ## 9 awes… 0 18 mbr000 AROME_Ar… 154 0.287 1.55 1.13 ## 10 awes… 3 18 mbr000 AROME_Ar… 170 0.830 3.05 2.05 ## # … with 350 more rows, and 1 more variable: stde &lt;dbl&gt; ## ## attr(,&quot;parameter&quot;) ## [1] &quot;T2m&quot; ## attr(,&quot;start_date&quot;) ## [1] &quot;2019021706&quot; ## attr(,&quot;end_date&quot;) ## [1] &quot;2019021718&quot; ## attr(,&quot;num_stations&quot;) ## [1] 180 But this only computes the scores for each forecast cycle and not for all of the cycles combined. We can rectify that by passing the groupings as a list with different grouping combinations verif_t2m &lt;- ens_verify( t2m, T2m, groupings = list(&quot;leadtime&quot;, c(&quot;leadtime&quot;, &quot;fcst_cycle&quot;)), thresholds = seq(262, 272, 2) ) If we pull out the fcst_cycle column, we will see that we now have a new entry for all cycles. pull(verif_t2m$ens_summary_scores, fcst_cycle) %&gt;% unique() ## [1] &quot; 06; 12; 18&quot; &quot;06&quot; &quot;12&quot; &quot;18&quot; So we can plot for all forecast cycles plot_point_verif( verif_t2m, brier_score_decomposition, facet_by = vars(fcst_cycle, threshold), num_facet_cols = 6 ) If we’ve computed some different verifications and we want to plot them together, we can join those score objects using bind_point_verif. We have done verification for 2m temperature and 10m wind speed, so we can bind those together. verif_s10m &lt;- ens_verify( s10m, S10m, thresholds = c(1, 2, 5, 8), groupings = list(&quot;leadtime&quot;, c(&quot;leadtime&quot;, &quot;fcst_cycle&quot;)) ) verif_all &lt;- bind_point_verif(verif_t2m, verif_s10m) And then we can plot those together with the same function plot_point_verif( verif_all, crps, facet_by = vars(parameter, fcst_cycle), num_facet_cols = 4 ) 4.3.6 Saving the verification data You can save your verification data with the function save_point_verif. You simply give it the data and the directory you want to save in. The filenames are generated from the data, so although you can provide a template, it is best not to. save_point_verif(verif_t2m, verif_path = here(&quot;data/verification&quot;)) save_point_verif(verif_s10m, verif_path = here(&quot;data/verification&quot;)) Once the data are saved you can use a browser based app to view and interact with the data. This app is created in the R package shiny, so to launch it you run the function shiny_plot_point_verif and give it the starting directory to look for data. shiny_plot_point_verif(here(&quot;data/verification&quot;)) 4.4 TO BE ADDED 4.4.1 vertical profiles 4.4.2 verification of vertical profiles 4.4.3 plot vertical verification 4.4.4 jitter forecast 4.4.5 conditional verification 4.4.6 joint probabilities (maybe) 4.4.7 bootstrapping 4.4.8 pooled bootstrapping "]
]
